{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression\n",
    " - Use when you want to predict target scalar values based on scalar value inputs; as opposed to predicting the class that an object belongs to\n",
    " - Regression examples: predicting tomorrow's temperature, or price of a car\n",
    " - To implement in network, we will have to change our output layer and loss calculation (error calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Activation:\n",
    "- this will be used for the output layer\n",
    "- it is just y = x, or outputs = inputs\n",
    "- derivative wrt to input is 1, so just passing back gradients of loss layer\n",
    "- this code really does nothing, we just do it for completeness and to have an ouput layer for better clarity in our code. It should add minimal amount to training time, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error Loss:\n",
    "- the average of the squared difference between the outputs and their respective ground truth values for a particular sample\n",
    "- Li = 1/J * sum((yij - y_hatij)^2); where Li = loss for sample i, J is the number of outputs, yij is ground truth, y_hatij is prediction\n",
    "- the further we get from the ground truth, the more harshly the network is penalized because the loss grows quadratically\n",
    "- Derivative (use chain rule): dLi/dy_hatij = d/dy_hatij ( 1/J * sum((yij - y_hatij)^2) ) = 1/J * d/dy_hatij sum((yij - y_hatij)^2) = 1/J * 2 (yij - y_hatij) * d/dy_hatij (yij - y_hatij) = 1/J * 2 (yij - y_hatij) * (0 - 1) = -2/J * (yij - y_hatij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error Loss code\n",
    "- forward pass: remeber, axis = -1 takes average of row (aka outputs for a sample)\n",
    "- backward pass: still doing gradient normalization via averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "        # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error Loss\n",
    "- average of the absolute value of the difference between ground truth and predicted values\n",
    "- Li = 1/J * abs(yij - y_hatij); same definitions as above\n",
    "- penalizes error linearly - error increases 1:1 the further you are from loss. Produces sparser results and is more robust to outliers. Sparsity means weights/biases that are 0, which can be beneficial for interpretability and feature selection, but can also make the model more unstable and passes up on incorporating that information. Can also be more computationally efficient and robust to outliers because of the 0s. Another general downside is that it is very sensitive to scaling of the training/testing data\n",
    "- MAE (L1) used less frequently than MSE (L2)\n",
    "- Derivative dLi/dy_hatij = d/dy_hatij ( 1/J * abs(yij - y_hatij) )=  1/J * d/dy_hatij * abs(yij - y_hatij); absolute value derivative 1 when term >= 0 or -1 when < 0; dLij = 1/J * {1; yij - y_hatij >= 0 or -1; yij - y_hatij < 0}; Remember that abs derivate is undefined at 0, but we just say it is 1 for ease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE Codes\n",
    "- Forward Pass: same -1 axis concept in the forward pass mean\n",
    "- Backward pass: use np.sign() to get the derivative of the abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between L1 (MAE) and L2 (MSE) (credit ChatGPT):\n",
    "1. **Robustness to Outliers**:\n",
    "   - L1 Loss: More robust to outliers because it doesn't square the errors. Outliers have a linear impact on the loss.\n",
    "   - L2 Loss: Sensitive to outliers due to the squaring of errors. Outliers have a quadratic impact on the loss, making it less robust.\n",
    "\n",
    "2. **Sparsity**:\n",
    "   - L1 Loss: Encourages sparsity in the solution, meaning many coefficients can be exactly zero, effectively performing feature selection.\n",
    "   - L2 Loss: Does not inherently encourage sparsity, leading to solutions where most coefficients are non-zero.\n",
    "\n",
    "3. **Computational Efficiency**:\n",
    "   - L1 Loss: Sparse solutions are computationally efficient because they involve fewer non-zero parameters.\n",
    "   - L2 Loss: May involve more computational overhead due to non-sparsity.\n",
    "\n",
    "4. **Solution Stability**:\n",
    "   - L1 Loss: Can lead to less stable solutions, especially when features are highly correlated, as it may arbitrarily select one feature over another.\n",
    "   - L2 Loss: Generally produces more stable solutions, particularly when features are correlated.\n",
    "\n",
    "5. **Impact of Scaling**:\n",
    "   - L1 Loss: Sensitive to feature scaling since it treats all errors equally regardless of their magnitude.\n",
    "   - L2 Loss: Less sensitive to feature scaling due to the squaring of errors, but can still be affected by extreme feature values.\n",
    "\n",
    "6. **Optimization Challenges**:\n",
    "   - L1 Loss: Introduces non-differentiability at zero, which can make optimization challenging, especially in large-scale problems.\n",
    "   - L2 Loss: Smooth and differentiable, making optimization relatively easier compared to L1 loss.\n",
    "\n",
    "7. **Performance on Small Datasets**:\n",
    "   - L1 Loss: May not perform well on small datasets where sparsity could be too aggressive, leading to underfitting.\n",
    "   - L2 Loss: Generally performs well on small datasets, providing more stable estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Calculation:\n",
    "- need a new way to calculate accuracy because cannot just check equivalence anymore. This will result in the model appearing to be very inaccurate, when in reality it is ok for something to just be close enough. Ex: ground truth = 100, predicted value = 99.999\n",
    "- to do this, we need to calculate a cushion value, where we will consider something correct if it falls within the cushion. \n",
    "- one way: take the standard deviation of the ground truth values in the dataset, and divide by some scalar. The larger the scalar, the more strict/smaller our cushion is.\n",
    "- then, variations from ground truth that are less than the cushion allowance are considered accurate/correct.\n",
    "- Code example below: y=groud truth of data set; 250 is our scalar for the cushion size. Then we take the absolute value of the differnece between ground predicted values and ground truth and do a boolean check that it is within our cushion. If it is the value is 1, if not, it is 0. Then we average the 1s and 0s for our accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_precision = np.std(y) / 250 #250 is the scalar\n",
    "\n",
    "predictions = activation2.output #network outputs\n",
    "accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
