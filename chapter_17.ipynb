{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression\n",
    " - Use when you want to predict target scalar values based on scalar value inputs; as opposed to predicting the class that an object belongs to\n",
    " - Regression examples: predicting tomorrow's temperature, or price of a car\n",
    " - To implement in network, we will have to change our output layer and loss calculation (error calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Activation:\n",
    "- this will be used for the output layer\n",
    "- it is just y = x, or outputs = inputs\n",
    "- derivative wrt to input is 1, so just passing back gradients of loss layer\n",
    "- this code really does nothing, we just do it for completeness and to have an ouput layer for better clarity in our code. It should add minimal amount to training time, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error Loss:\n",
    "- the average of the squared difference between the outputs and their respective ground truth values for a particular sample\n",
    "- Li = 1/J * sum((yij - y_hatij)^2); where Li = loss for sample i, J is the number of outputs, yij is ground truth, y_hatij is prediction\n",
    "- the further we get from the ground truth, the more harshly the network is penalized because the loss grows quadratically\n",
    "- Derivative (use chain rule): dLi/dy_hatij = d/dy_hatij ( 1/J * sum((yij - y_hatij)^2) ) = 1/J * d/dy_hatij sum((yij - y_hatij)^2) = 1/J * 2 (yij - y_hatij) * d/dy_hatij (yij - y_hatij) = 1/J * 2 (yij - y_hatij) * (0 - 1) = -2/J * (yij - y_hatij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Squared Error Loss code\n",
    "- forward pass: remeber, axis = -1 takes average of row (aka outputs for a sample)\n",
    "- backward pass: still doing gradient normalization via averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss): # L2 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "        # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error Loss\n",
    "- average of the absolute value of the difference between ground truth and predicted values\n",
    "- Li = 1/J * abs(yij - y_hatij); same definitions as above\n",
    "- penalizes error linearly - error increases 1:1 the further you are from loss. Produces sparser results and is more robust to outliers. Sparsity means weights/biases that are 0, which can be beneficial for interpretability and feature selection, but can also make the model more unstable and passes up on incorporating that information. Can also be more computationally efficient and robust to outliers because of the 0s. Another general downside is that it is very sensitive to scaling of the training/testing data\n",
    "- MAE (L1) used less frequently than MSE (L2)\n",
    "- Derivative dLi/dy_hatij = d/dy_hatij ( 1/J * abs(yij - y_hatij) )=  1/J * d/dy_hatij * abs(yij - y_hatij); absolute value derivative 1 when term >= 0 or -1 when < 0; dLij = 1/J * {1; yij - y_hatij >= 0 or -1; yij - y_hatij < 0}; Remember that abs derivate is undefined at 0, but we just say it is 1 for ease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE Codes\n",
    "- Forward Pass: same -1 axis concept in the forward pass mean\n",
    "- Backward pass: use np.sign() to get the derivative of the abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between L1 (MAE) and L2 (MSE) (credit ChatGPT):\n",
    "1. **Robustness to Outliers**:\n",
    "   - L1 Loss: More robust to outliers because it doesn't square the errors. Outliers have a linear impact on the loss.\n",
    "   - L2 Loss: Sensitive to outliers due to the squaring of errors. Outliers have a quadratic impact on the loss, making it less robust.\n",
    "\n",
    "2. **Sparsity**:\n",
    "   - L1 Loss: Encourages sparsity in the solution, meaning many coefficients can be exactly zero, effectively performing feature selection.\n",
    "   - L2 Loss: Does not inherently encourage sparsity, leading to solutions where most coefficients are non-zero.\n",
    "\n",
    "3. **Computational Efficiency**:\n",
    "   - L1 Loss: Sparse solutions are computationally efficient because they involve fewer non-zero parameters.\n",
    "   - L2 Loss: May involve more computational overhead due to non-sparsity.\n",
    "\n",
    "4. **Solution Stability**:\n",
    "   - L1 Loss: Can lead to less stable solutions, especially when features are highly correlated, as it may arbitrarily select one feature over another.\n",
    "   - L2 Loss: Generally produces more stable solutions, particularly when features are correlated.\n",
    "\n",
    "5. **Impact of Scaling**:\n",
    "   - L1 Loss: Sensitive to feature scaling since it treats all errors equally regardless of their magnitude.\n",
    "   - L2 Loss: Less sensitive to feature scaling due to the squaring of errors, but can still be affected by extreme feature values.\n",
    "\n",
    "6. **Optimization Challenges**:\n",
    "   - L1 Loss: Introduces non-differentiability at zero, which can make optimization challenging, especially in large-scale problems.\n",
    "   - L2 Loss: Smooth and differentiable, making optimization relatively easier compared to L1 loss.\n",
    "\n",
    "7. **Performance on Small Datasets**:\n",
    "   - L1 Loss: May not perform well on small datasets where sparsity could be too aggressive, leading to underfitting.\n",
    "   - L2 Loss: Generally performs well on small datasets, providing more stable estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Calculation:\n",
    "- need a new way to calculate accuracy because cannot just check equivalence anymore. This will result in the model appearing to be very inaccurate, when in reality it is ok for something to just be close enough. Ex: ground truth = 100, predicted value = 99.999\n",
    "- to do this, we need to calculate a cushion value, where we will consider something correct if it falls within the cushion. \n",
    "- one way: take the standard deviation of the ground truth values in the dataset, and divide by some scalar. The larger the scalar, the more strict/smaller our cushion is.\n",
    "- then, variations from ground truth that are less than the cushion allowance are considered accurate/correct.\n",
    "- Code example below: y=groud truth of data set; 250 is our scalar for the cushion size. Then we take the absolute value of the differnece between ground predicted values and ground truth and do a boolean check that it is within our cushion. If it is the value is 1, if not, it is 0. Then we average the 1s and 0s for our accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_precision = np.std(y) / 250 #250 is the scalar\n",
    "\n",
    "predictions = activation2.output #network outputs\n",
    "accuracy = np.mean(np.absolute(predictions - y) < accuracy_precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Code\n",
    "- RUN THE LAST CELL IN WORKBOOK FOR THIS CODE TO WORK PROPERLY\n",
    "- the version shown below is the best performing example from the book; however, earlier examples perform much worse. They are discussed below.\n",
    "- Firstly, notice that for the accuracy precision cushion, we establish accuracy_precision variable at the end of all the initialization, then we calculate the accuracy during the loop\n",
    "- Through the book we run the regression with 2 layers, then 3\n",
    "- the first example in the book results: (2 layers, accuracy <1%)\n",
    "- the final example - 3 layers with Optimizer_Adam(learning_rate=0.005, decay=1e-3); performs very well\n",
    "- Note: experienced a lot of issues transcribing this code, so it is from the github and may not 100% match book if there were any changes from the book version to github version, but I couldn't find my error\n",
    "- Also, discussed further below, but we changed the weight intialization draw from .01 * normal distribution to .1, for more accurate performance, but I found that I got much better performance with .01 and the default adam optimizer parameters. So not really sure what is going on there. I left it at .1 to stay consistent with book. I was able to replicate the results of the book at their settings.\n",
    "- with book settings at a weight intialization scalar of .1, 3 layers with Optimizer_Adam(learning_rate=0.005, decay=1e-3), we get accuracy 87.7. It doesn't match performance of book - but I note that was able to get high 90s with scalar of .01 rather than .1\n",
    "- Overall, there is a lot of jumping in the accuracy, which may show the the learning rate is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.484 (data_loss: 0.484, reg_loss: 0.000), lr: 0.005\n",
      "epoch: 100, acc: 0.007, loss: 0.061 (data_loss: 0.061, reg_loss: 0.000), lr: 0.004549590536851684\n",
      "epoch: 200, acc: 0.231, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.004170141784820684\n",
      "epoch: 300, acc: 0.018, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.003849114703618168\n",
      "epoch: 400, acc: 0.485, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0035739814152966403\n",
      "epoch: 500, acc: 0.643, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00333555703802535\n",
      "epoch: 600, acc: 0.658, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0031269543464665416\n",
      "epoch: 700, acc: 0.684, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002942907592701589\n",
      "epoch: 800, acc: 0.697, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0027793218454697055\n",
      "epoch: 900, acc: 0.698, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0026329647182727752\n",
      "epoch: 1000, acc: 0.061, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002501250625312656\n",
      "epoch: 1100, acc: 0.718, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0023820867079561697\n",
      "epoch: 1200, acc: 0.455, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002273760800363802\n",
      "epoch: 1300, acc: 0.740, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002174858634188778\n",
      "epoch: 1400, acc: 0.038, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020842017507294707\n",
      "epoch: 1500, acc: 0.751, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020008003201280513\n",
      "epoch: 1600, acc: 0.759, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001923816852635629\n",
      "epoch: 1700, acc: 0.774, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001852537977028529\n",
      "epoch: 1800, acc: 0.261, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017863522686673815\n",
      "epoch: 1900, acc: 0.786, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017247326664367024\n",
      "epoch: 2000, acc: 0.321, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016672224074691564\n",
      "epoch: 2100, acc: 0.793, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016134236850596968\n",
      "epoch: 2200, acc: 0.035, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015629884338855893\n",
      "epoch: 2300, acc: 0.797, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015156107911488332\n",
      "epoch: 2400, acc: 0.796, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014710208884966167\n",
      "epoch: 2500, acc: 0.716, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014289797084881396\n",
      "epoch: 2600, acc: 0.802, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001389274798555154\n",
      "epoch: 2700, acc: 0.800, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013517166801838335\n",
      "epoch: 2800, acc: 0.801, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013161358252171624\n",
      "epoch: 2900, acc: 0.806, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012823800974608873\n",
      "epoch: 3000, acc: 0.079, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012503125781445363\n",
      "epoch: 3100, acc: 0.809, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012198097096852891\n",
      "epoch: 3200, acc: 0.810, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011907597046915933\n",
      "epoch: 3300, acc: 0.810, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011630611770179114\n",
      "epoch: 3400, acc: 0.814, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011366219595362584\n",
      "epoch: 3500, acc: 0.811, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011113580795732384\n",
      "epoch: 3600, acc: 0.821, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010871928680147858\n",
      "epoch: 3700, acc: 0.818, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010640561821664183\n",
      "epoch: 3800, acc: 0.817, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010418837257762034\n",
      "epoch: 3900, acc: 0.817, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010206164523372118\n",
      "epoch: 4000, acc: 0.818, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010002000400080014\n",
      "epoch: 4100, acc: 0.822, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009805844283192783\n",
      "epoch: 4200, acc: 0.794, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009617234083477593\n",
      "epoch: 4300, acc: 0.822, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009435742592942063\n",
      "epoch: 4400, acc: 0.825, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009260974254491572\n",
      "epoch: 4500, acc: 0.827, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009092562284051646\n",
      "epoch: 4600, acc: 0.827, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000893016610108948\n",
      "epoch: 4700, acc: 0.828, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008773469029654326\n",
      "epoch: 4800, acc: 0.788, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000862217623728229\n",
      "epoch: 4900, acc: 0.832, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008476012883539582\n",
      "epoch: 5000, acc: 0.833, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008334722453742291\n",
      "epoch: 5100, acc: 0.832, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008198065256599442\n",
      "epoch: 5200, acc: 0.832, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008065817067268914\n",
      "epoch: 5300, acc: 0.832, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007937767899666614\n",
      "epoch: 5400, acc: 0.834, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007813720893889669\n",
      "epoch: 5500, acc: 0.834, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007693491306354824\n",
      "epoch: 5600, acc: 0.846, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007576905591756327\n",
      "epoch: 5700, acc: 0.839, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007463800567248844\n",
      "epoch: 5800, acc: 0.188, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007354022650389764\n",
      "epoch: 5900, acc: 0.841, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007247427163357008\n",
      "epoch: 6000, acc: 0.807, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000714387769681383\n",
      "epoch: 6100, acc: 0.849, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007043245527539089\n",
      "epoch: 6200, acc: 0.848, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006945409084595084\n",
      "epoch: 6300, acc: 0.852, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006850253459377996\n",
      "epoch: 6400, acc: 0.846, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006757669955399379\n",
      "epoch: 6500, acc: 0.845, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006667555674089878\n",
      "epoch: 6600, acc: 0.313, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006579813133307014\n",
      "epoch: 6700, acc: 0.854, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006494349915573451\n",
      "epoch: 6800, acc: 0.856, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006411078343377356\n",
      "epoch: 6900, acc: 0.854, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00063299151791366\n",
      "epoch: 7000, acc: 0.832, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006250781347668457\n",
      "epoch: 7100, acc: 0.861, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006173601679219657\n",
      "epoch: 7200, acc: 0.860, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006098304671301379\n",
      "epoch: 7300, acc: 0.713, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006024822267743102\n",
      "epoch: 7400, acc: 0.863, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005953089653530181\n",
      "epoch: 7500, acc: 0.863, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000588304506412519\n",
      "epoch: 7600, acc: 0.856, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005814629608093965\n",
      "epoch: 7700, acc: 0.864, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005747787101965744\n",
      "epoch: 7800, acc: 0.864, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005682463916354131\n",
      "epoch: 7900, acc: 0.838, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005618608832453085\n",
      "epoch: 8000, acc: 0.864, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00055561729081009\n",
      "epoch: 8100, acc: 0.864, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005495109352676119\n",
      "epoch: 8200, acc: 0.858, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005435373410153278\n",
      "epoch: 8300, acc: 0.865, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005376922249704269\n",
      "epoch: 8400, acc: 0.867, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005319714863283328\n",
      "epoch: 8500, acc: 0.868, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005263711969681019\n",
      "epoch: 8600, acc: 0.868, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005208875924575476\n",
      "epoch: 8700, acc: 0.866, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005155170636148056\n",
      "epoch: 8800, acc: 0.869, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005102561485865905\n",
      "epoch: 8900, acc: 0.871, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005051015254066068\n",
      "epoch: 9000, acc: 0.382, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005000500050005\n",
      "epoch: 9100, acc: 0.870, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004950985246063966\n",
      "epoch: 9200, acc: 0.870, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004902441415825081\n",
      "epoch: 9300, acc: 0.871, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004854840275754928\n",
      "epoch: 9400, acc: 0.873, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004808154630252909\n",
      "epoch: 9500, acc: 0.869, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047623583198399844\n",
      "epoch: 9600, acc: 0.872, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047174261722804036\n",
      "epoch: 9700, acc: 0.875, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046733339564445275\n",
      "epoch: 9800, acc: 0.875, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046300583387350687\n",
      "epoch: 9900, acc: 0.879, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045875768419121016\n",
      "epoch: 10000, acc: 0.877, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045458678061641964\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# Create Dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 64 output values\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Create third Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Linear activation:\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
    "\n",
    "\n",
    "# Accuracy precision for accuracy calculation\n",
    "# There are no really accuracy factor for regression problem,\n",
    "# but we can simulate/approximate it. We'll calculate it by checking\n",
    "# how many values have a difference to their ground truth equivalent\n",
    "# less than given precision\n",
    "# We'll calculate this precision as a fraction of standard deviation\n",
    "# of all the ground truth values\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Perform a forward pass through third Dense layer\n",
    "    # takes outputs of activation function of second layer as inputs\n",
    "    dense3.forward(activation2.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of third dense layer here\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2) + \\\n",
    "        loss_function.regularization_loss(dense3)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # To calculate it we're taking absolute difference between\n",
    "    # predictions and ground truth values and compare if differences\n",
    "    # are lower than given precision value\n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) <\n",
    "                       accuracy_precision)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of The Testing Data to See Performance\n",
    "- in the first version of the model, the data is fit very poorly (2 layers, accuracy <1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test, y_test = sine_data()\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance of Weight Initialization:\n",
    " - weight initialization is important can result in a model fitting pretty well at default optimizer settings vs. not fitting at all\n",
    " - the book notes that if you change the weight initializion from .01 * random distriution to .1, then the model learns much better at any adam initialization\n",
    " - Example of weight intializer: Glorot Uniform\n",
    " - Glorot Uniform randomly draws weight intializations from a uniform distribtion\n",
    "  1) the limits (edges) of the uniform distribution is defined as limits = sqrt(6 / (fan_in+fan_out)). Where fan_in is the number of inputs in the weight tensor(array) and fan_out is the number of outputs in the weight tensor. So we will draw weight values from [-limit, limit]. These can be scaled by any amount (e.g., .01 or 1)\n",
    " - in the book, we just adjust the scaling of the weight intialization on a normal distribution by changing it from .01 to .1, which is different than glorot because glorot distribution changes with the number of input and output neurons, ours does not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Model Objects Up To This Point\n",
    "- Run this cell for the Regression cell above to run\n",
    "- dropped in completely fresh code from NNFS github\n",
    "- Note that we changed the self.weights intialization from .01 to .1, because this results in better performance on the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "\n",
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                   np.sum(layer.weights * \\\n",
    "                                          layer.weights)\n",
    "\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                   np.sum(layer.biases * \\\n",
    "                                          layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):  # L2 loss\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):  # L1 loss\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
