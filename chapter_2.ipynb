{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1]\n",
    "bias = 2\n",
    "\n",
    "output = (inputs[0] * weights[0]\n",
    "           + inputs[1] * weights[1]\n",
    "           + inputs[2] * weights[2]\n",
    "           + inputs[3] * weights[3] \n",
    "           + bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers from Scratch\n",
    "- A crude example of a fully connected network i.e. all neurons have connections to every other neuron. This is opposed to sparsely connected networks, which neurons are not connected at every layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [.5, -.91, .26, -.5]\n",
    "weights3 = [-.26, -.27, .17, .87]\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = .5\n",
    "\n",
    "output = [inputs[0] * weights1[0]\n",
    "           + inputs[1] * weights1[1]\n",
    "           + inputs[2] * weights1[2]\n",
    "           + inputs[3] * weights1[3] + bias1,\n",
    "           inputs[0] * weights2[0]\n",
    "           + inputs[1] * weights2[1]\n",
    "           + inputs[2] * weights2[2]\n",
    "           + inputs[3] * weights2[3] + bias2,\n",
    "           inputs[0] * weights3[0]\n",
    "           + inputs[1] * weights3[1]\n",
    "           + inputs[2] * weights3[2]\n",
    "           + inputs[3] * weights3[3] + bias3,\n",
    "           ]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A More Dynamic Implementation\n",
    "- iterate over the weights in biases using zip, in other words: for each neuron. Three sets of weights and three biases = 3 neurons\n",
    "- integrate (add up) the output of the neuron over each weight times input then add the bias\n",
    "- append each to the layer output list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1],[0.5, -0.91, 0.26, -0.5],[-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, .5]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input * weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix, Array, Tensors\n",
    "-Terms are used somewhat interchangeably when it comes to ML/AI\n",
    "-Arrays are homologous - meaning for each row, the row length is identical, or for each column the column length is identical - this is along 2 dimensions. Generally, there must be consistency across dimensions\n",
    "- Arrays defined as row count, column count : 3x2, (3,2) => 3 rows 2 columns\n",
    "- Matrix is a subset of arrays => two dimensional homologous array.\n",
    "- Determine array size of dimensions by counting the number of elements inside each braket (see example below)\n",
    "- Tensors are not just arrays, but are represented as arrays for coding purposes\n",
    "- linear array = 1D array\n",
    "- Vector = 1D array\n",
    "- My own research shows that tensor is a phrase used for arrays >= 3D; others say it is a generalization of scalars and vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_homologous_array = [[4,2,3],\n",
    "                        [5,1]]\n",
    "\n",
    "matrix_and_array = [[4,2],\n",
    "                    [5,1],\n",
    "                    [8,2]]\n",
    "\n",
    "#first level of this array is a list of three lists => 1st dimension is 3\n",
    "#second level of this array is a list of two lists => 2nd dimension is 2\n",
    "#third level of this array is a list of 4 integers => 3rd dimension is 4\n",
    "lolol = [[[1,5,6,2],\n",
    "          [3,2,1,3]],\n",
    "         [[5,2,1,2],\n",
    "           [6,4,8,4]],\n",
    "         [[2,8,5,3],\n",
    "           [1,1,9,4]]]\n",
    "#array size is (3,2,4) => 3D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot Product and Vector Addition\n",
    "- Dot products between (2) vectors result in a scalar, a single value, because vectors are 1D\n",
    "- Dot product of vectors is sum of the products of consecutive elements between two arrays\n",
    "===> this is the operation needed to multiply weights and neuron inputs\n",
    "- Vector addition is done element wise, each item in one vector is added to the same respective position in another vector \n",
    "====>this is how biases are added in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3] #aka inputs to neurons\n",
    "b = [2, 3, 4] #aka weights of neurons\n",
    "\n",
    "dot_product = a[0]*b[0] + a[1]*b[1] + a[2]*b[2]\n",
    "print(dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Single Neuron with NumPy\n",
    "- using dot products and vector addition to calculate final neuron output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "outputs = np.dot(weights,inputs) + bias\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Layer of Neurons with NumPy\n",
    "- multiple neurons recieving same inputs (e.g., a layer)\n",
    "- changes from a vector (1D) to a matrix (2D), because now have a list of list of weights => a set of weights for each Neuron in the layer\n",
    "- Do the dot product of weights matrix and input vector => now results in a vector rather than scalar because multiple neurons result in multiple scalars. Vector size will be number of neurons, allows next bullet point below to work\n",
    "- Add biases element wise to the resulting scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5] #four inputs\n",
    "weights = [[0.2, 0.8, -0.5, 1], #three neurons with four weights for each input\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2.0, 3.0, 0.5] #three biases for three neurons\n",
    "\n",
    "#numpy automatically converts lists above to arrays\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "#Matrix multiplication is a dot product of the first matrix's row and the second matrix's first column. \n",
    "#So because each row represents neuron weights in the weights matrix and we want to do the dot product\n",
    "#of the weights and inputs, weights should be first inputs should be second. \n",
    "#also works because weights have matrix size (3,4) and inputs size is (4,), so row len matches col len\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Size\n",
    "- Neural nets are typically trained in batches - or groups of multiple samples (or observations) of various features in a feature set\n",
    "= sample = obeservation = feature set instance\n",
    "= In my understanding: feature set => the independent variables/attributes/properties used to model the dependent variable\n",
    "-Using small batch sizes causes the gradient updates/fitting aka convergence to bounce around because less samples are used to estimate gradient. Using more samples has a more direct convergence, but at the cost of: (1) more memory usage and (2) slow down associated with computing gradient for a greater number of samples\n",
    "\n",
    "No code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix Product\n",
    "- Matrix product is the dot product of each matrix row and each matrix column, resulting in a matrix with the same dimensons as the number of rows in the first matrix and columns in the second matrix\n",
    "- Requires that the first matrix have the same number of colunms as the second matrix does rows, so that the dot product works\n",
    "- row/column vectors can be thought of as matrix with size of 1 for row/column, matrix product results in matrix of size 1,1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposition for the Matrix Product\n",
    "- rotates matrix so that rows become columns and columns rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#row vector - note without second set of brackets this would be a 1D array\n",
    "np.array([[1,2,3]]) #has shape of (1,3) => if no second brackets shape would be (3,) \n",
    "\n",
    "#column vector- just transpose row vector\n",
    "np.array([[1,2,3]]).T\n",
    "#to construct column vector manually, would need to make a list of lists where each inner list consists of a single value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = [2, 3, 4]\n",
    "\n",
    "a = np.array([a]) #without second set of brackest these would be 1D\n",
    "b = np.array([b]).T\n",
    "\n",
    "print(np.dot(a, b)) #1,1 matrix returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Layer of Neurons & Batch of Data w/ NumPy\n",
    "- previously were able to perform dot on single row of inputs and 3 sets of neuron weights because input was 1D vector\n",
    "- when have a batch of inputs and multiple weights, where each input row represents a sample and each weight row represents a neuron's weights, cannot perform the matrix (dot) product directly on them => makes sense because, assuing inputs are first term and weights are second term and matrix are same shape, multiplying the row times the column would not be multiplying the neuron weights times the respective inputs. Instead would be treating each column as its own neuron, which is wrong and would result in the wrong output. => if matrix are not same shape wouldn't work either because there would be term mismatch\n",
    "- solve this by transposing the weights matrix so that now the rows, which represented neurons, are now columns. So now dot product is operating on input row and weight.T column, giving us the sum of the weights time the inputs for each neuron. Resulting matrix will have each column representing the output of each neuron for a particular sample (input row), and each row corresponding to each input sample.\n",
    "- keeping the output row oriented makes it easier to pass one layer's output as the input to the next layer => in other words, each input row aka sample corresponds to an output row of that sample transformed by the layer, so the output can be passed into the next layer using the same method\n",
    "- bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5] #added to each row of output\n",
    "\n",
    "layer_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "print(layer_outputs) \n",
    "#notice that as discussed before, the rows of the output correspond with the layer's output for that input, where each value in the row is a neurons output\n",
    "#so the third value in the first row of the output is the third neuron's output of the first input (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
