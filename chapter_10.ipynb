{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "- Stochastic Gradient Descent - fitting single sample at a time\n",
    "- vanilla gradient descent, gradient descent, batch gradient descent - fitting whole dataset at once\n",
    "- mini batch dataset - fit smaller (mini) batches of data instead of all data at once\n",
    "- These terms can get confusing. For the purpose of the book we will call mini batches batches \n",
    "- some call it stochastic gradient descent regardless of batch size/single sample\n",
    "\n",
    "- to implement gradient descent, need a learing rate and the calculated gradients of loss function with respect to parameters. To get the parameter update amounts just multiply -learning rate * gradients, then add to the parameters. Learning rate is negated because we are trying to find minimum, so stepping towards lowest loss\n",
    "- see optimizer object below - in previous work we stored the layer weights and biases gradients in the layer objects as attributes so we can now make use of & modify them via the optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic network training using SGD optimizer\n",
    "- epoch - a full pass through the training data, including forwards and backwards\n",
    "- typically models are trained over multiple epochs, but obviously the less the better\n",
    "- here it is implemented via for loop where each epoch is 1 interation of the loop, including a forward and backward pass, so do that 10000x\n",
    "- for this intial run, we chose a learning rate of 1, (it is the default)\n",
    "- run the final cell in this workbook with full network code to use this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099\n",
      "epoch: 100, acc: 0.400, loss: 1.087\n",
      "epoch: 200, acc: 0.417, loss: 1.077\n",
      "epoch: 300, acc: 0.413, loss: 1.076\n",
      "epoch: 400, acc: 0.400, loss: 1.074\n",
      "epoch: 500, acc: 0.403, loss: 1.071\n",
      "epoch: 600, acc: 0.417, loss: 1.067\n",
      "epoch: 700, acc: 0.440, loss: 1.062\n",
      "epoch: 800, acc: 0.457, loss: 1.055\n",
      "epoch: 900, acc: 0.410, loss: 1.062\n",
      "epoch: 1000, acc: 0.407, loss: 1.058\n",
      "epoch: 1100, acc: 0.407, loss: 1.057\n",
      "epoch: 1200, acc: 0.403, loss: 1.064\n",
      "epoch: 1300, acc: 0.427, loss: 1.051\n",
      "epoch: 1400, acc: 0.443, loss: 1.067\n",
      "epoch: 1500, acc: 0.400, loss: 1.058\n",
      "epoch: 1600, acc: 0.420, loss: 1.070\n",
      "epoch: 1700, acc: 0.410, loss: 1.049\n",
      "epoch: 1800, acc: 0.460, loss: 1.040\n",
      "epoch: 1900, acc: 0.483, loss: 1.033\n",
      "epoch: 2000, acc: 0.403, loss: 1.038\n",
      "epoch: 2100, acc: 0.447, loss: 1.022\n",
      "epoch: 2200, acc: 0.467, loss: 1.023\n",
      "epoch: 2300, acc: 0.437, loss: 1.005\n",
      "epoch: 2400, acc: 0.497, loss: 0.993\n",
      "epoch: 2500, acc: 0.513, loss: 0.981\n",
      "epoch: 2600, acc: 0.453, loss: 0.991\n",
      "epoch: 2700, acc: 0.513, loss: 0.980\n",
      "epoch: 2800, acc: 0.523, loss: 0.985\n",
      "epoch: 2900, acc: 0.507, loss: 0.993\n",
      "epoch: 3000, acc: 0.483, loss: 0.973\n",
      "epoch: 3100, acc: 0.493, loss: 0.973\n",
      "epoch: 3200, acc: 0.463, loss: 0.987\n",
      "epoch: 3300, acc: 0.510, loss: 0.992\n",
      "epoch: 3400, acc: 0.533, loss: 0.969\n",
      "epoch: 3500, acc: 0.547, loss: 0.994\n",
      "epoch: 3600, acc: 0.487, loss: 0.966\n",
      "epoch: 3700, acc: 0.493, loss: 0.969\n",
      "epoch: 3800, acc: 0.510, loss: 0.971\n",
      "epoch: 3900, acc: 0.540, loss: 0.989\n",
      "epoch: 4000, acc: 0.533, loss: 0.956\n",
      "epoch: 4100, acc: 0.567, loss: 0.988\n",
      "epoch: 4200, acc: 0.497, loss: 0.958\n",
      "epoch: 4300, acc: 0.500, loss: 0.972\n",
      "epoch: 4400, acc: 0.520, loss: 0.968\n",
      "epoch: 4500, acc: 0.550, loss: 0.984\n",
      "epoch: 4600, acc: 0.543, loss: 0.955\n",
      "epoch: 4700, acc: 0.563, loss: 1.006\n",
      "epoch: 4800, acc: 0.503, loss: 0.966\n",
      "epoch: 4900, acc: 0.483, loss: 0.965\n",
      "epoch: 5000, acc: 0.510, loss: 0.985\n",
      "epoch: 5100, acc: 0.553, loss: 0.953\n",
      "epoch: 5200, acc: 0.573, loss: 0.994\n",
      "epoch: 5300, acc: 0.500, loss: 0.969\n",
      "epoch: 5400, acc: 0.503, loss: 0.958\n",
      "epoch: 5500, acc: 0.517, loss: 0.979\n",
      "epoch: 5600, acc: 0.553, loss: 0.947\n",
      "epoch: 5700, acc: 0.567, loss: 0.959\n",
      "epoch: 5800, acc: 0.583, loss: 0.970\n",
      "epoch: 5900, acc: 0.547, loss: 0.950\n",
      "epoch: 6000, acc: 0.517, loss: 0.931\n",
      "epoch: 6100, acc: 0.577, loss: 0.966\n",
      "epoch: 6200, acc: 0.560, loss: 0.923\n",
      "epoch: 6300, acc: 0.590, loss: 0.955\n",
      "epoch: 6400, acc: 0.550, loss: 0.926\n",
      "epoch: 6500, acc: 0.547, loss: 0.926\n",
      "epoch: 6600, acc: 0.563, loss: 0.923\n",
      "epoch: 6700, acc: 0.593, loss: 0.875\n",
      "epoch: 6800, acc: 0.587, loss: 0.928\n",
      "epoch: 6900, acc: 0.613, loss: 0.874\n",
      "epoch: 7000, acc: 0.580, loss: 0.884\n",
      "epoch: 7100, acc: 0.610, loss: 0.859\n",
      "epoch: 7200, acc: 0.583, loss: 0.881\n",
      "epoch: 7300, acc: 0.623, loss: 0.912\n",
      "epoch: 7400, acc: 0.620, loss: 0.852\n",
      "epoch: 7500, acc: 0.617, loss: 0.877\n",
      "epoch: 7600, acc: 0.570, loss: 0.884\n",
      "epoch: 7700, acc: 0.597, loss: 0.840\n",
      "epoch: 7800, acc: 0.583, loss: 0.889\n",
      "epoch: 7900, acc: 0.643, loss: 0.897\n",
      "epoch: 8000, acc: 0.643, loss: 0.840\n",
      "epoch: 8100, acc: 0.630, loss: 0.854\n",
      "epoch: 8200, acc: 0.630, loss: 0.858\n",
      "epoch: 8300, acc: 0.587, loss: 0.873\n",
      "epoch: 8400, acc: 0.577, loss: 0.873\n",
      "epoch: 8500, acc: 0.607, loss: 0.874\n",
      "epoch: 8600, acc: 0.633, loss: 0.873\n",
      "epoch: 8700, acc: 0.617, loss: 0.845\n",
      "epoch: 8800, acc: 0.610, loss: 0.835\n",
      "epoch: 8900, acc: 0.617, loss: 0.846\n",
      "epoch: 9000, acc: 0.610, loss: 0.870\n",
      "epoch: 9100, acc: 0.603, loss: 0.934\n",
      "epoch: 9200, acc: 0.623, loss: 0.905\n",
      "epoch: 9300, acc: 0.643, loss: 0.866\n",
      "epoch: 9400, acc: 0.643, loss: 0.837\n",
      "epoch: 9500, acc: 0.590, loss: 0.865\n",
      "epoch: 9600, acc: 0.627, loss: 0.863\n",
      "epoch: 9700, acc: 0.630, loss: 0.830\n",
      "epoch: 9800, acc: 0.663, loss: 0.844\n",
      "epoch: 9900, acc: 0.627, loss: 0.820\n",
      "epoch: 10000, acc: 0.633, loss: 0.848\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "nnfs.init()\n",
    "\n",
    "###NOTE: run full network code in final cell of this workbook so that this example works\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100: #only every hundredth epoch\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the output of this intial training method showed some learing (accuracy ~63%), but loss did not drop \n",
    "- the visualiztion of this training: https://nnfs.io/pup\n",
    "- in the visualization, there is a \"flashy wiggle\" effect, indicating the learning rate may be too high. We can also tell via the loss, which does not decrease smoothly, but bounces around, (i.e. the loss decreases between epochs, then increases again between other epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Learning Rate\n",
    "- in most cases applying the full negative gradient to the parameters is too big of a step, as the gradient is continously changing (and we are applying a point estimate of a tangent function). So a big jump may result in jumping over the steepest areas of the function instead of more closely hugging to the function's curvature.\n",
    "- small steps make sure we follow the direction of steepest descent, hugging more closely to the function curve by not jumping too much. But too small is bad too - takes longer to arive at optimal parameters and more prone to getting stuck in local minimums, the lowest point of a function in a given x range (vs global minimum - the lowest possible y value a function can output)\n",
    "- ideal global minimum for ANNs is a loss of 0 - however this is typically not achieved. You know you are stuck in some local minumum if the loss is not low/close to 0\n",
    "- Gradient descent algo follows the direction of steepest descent, no matter how large or small it is. So if you are near a local minimum, this is what causes it to get stcuk because graidents near local minimum are lower, causing smaller parameter adjustment\n",
    "- too low learning rate can cause learning stagnation - stuck in local minimum\n",
    "- too high learning rate can cause gradient explosion - where gradient updates cause model loss to rise instead of fall. Eventually loss/gradients become so big that they cannot be stored in floating point, causing error. Can be costly if model has taken a while to train, then waste of time and computing resources\n",
    "- with just learning rate, need to select an learning rate that is not too high or too low for the reasons discussed above. Too high and will cause loss to bounce around/gradient explosion, too low and the model will learn too slow.\n",
    "- alternatives to just setting learning rate are learning rate decay and momentum\n",
    "- setting these factors are called hyper parameters, and setting them appropriately requires experience, it is difficult to prescribe anything. Have to see how model performs with different hyperparameters and adjust from there.\n",
    "- see code example below for lower learning rate - BE SURE TO RUN FINAL CELL SO THE OBJECTS WORK\n",
    "- in the code example, setting learning rate to .85 causes slightly higher accuracy, and slightly lower loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099\n",
      "epoch: 100, acc: 0.403, loss: 1.091\n",
      "epoch: 200, acc: 0.410, loss: 1.078\n",
      "epoch: 300, acc: 0.423, loss: 1.077\n",
      "epoch: 400, acc: 0.413, loss: 1.075\n",
      "epoch: 500, acc: 0.400, loss: 1.074\n",
      "epoch: 600, acc: 0.410, loss: 1.071\n",
      "epoch: 700, acc: 0.417, loss: 1.067\n",
      "epoch: 800, acc: 0.440, loss: 1.064\n",
      "epoch: 900, acc: 0.443, loss: 1.057\n",
      "epoch: 1000, acc: 0.420, loss: 1.050\n",
      "epoch: 1100, acc: 0.397, loss: 1.061\n",
      "epoch: 1200, acc: 0.387, loss: 1.060\n",
      "epoch: 1300, acc: 0.420, loss: 1.061\n",
      "epoch: 1400, acc: 0.460, loss: 1.055\n",
      "epoch: 1500, acc: 0.390, loss: 1.057\n",
      "epoch: 1600, acc: 0.450, loss: 1.072\n",
      "epoch: 1700, acc: 0.400, loss: 1.049\n",
      "epoch: 1800, acc: 0.423, loss: 1.039\n",
      "epoch: 1900, acc: 0.387, loss: 1.059\n",
      "epoch: 2000, acc: 0.437, loss: 1.053\n",
      "epoch: 2100, acc: 0.443, loss: 1.026\n",
      "epoch: 2200, acc: 0.377, loss: 1.050\n",
      "epoch: 2300, acc: 0.433, loss: 1.016\n",
      "epoch: 2400, acc: 0.460, loss: 1.000\n",
      "epoch: 2500, acc: 0.493, loss: 1.010\n",
      "epoch: 2600, acc: 0.527, loss: 0.998\n",
      "epoch: 2700, acc: 0.523, loss: 0.977\n",
      "epoch: 2800, acc: 0.507, loss: 0.967\n",
      "epoch: 2900, acc: 0.487, loss: 0.993\n",
      "epoch: 3000, acc: 0.520, loss: 0.987\n",
      "epoch: 3100, acc: 0.540, loss: 0.980\n",
      "epoch: 3200, acc: 0.543, loss: 0.950\n",
      "epoch: 3300, acc: 0.530, loss: 0.952\n",
      "epoch: 3400, acc: 0.543, loss: 0.970\n",
      "epoch: 3500, acc: 0.530, loss: 0.942\n",
      "epoch: 3600, acc: 0.543, loss: 0.955\n",
      "epoch: 3700, acc: 0.557, loss: 0.961\n",
      "epoch: 3800, acc: 0.543, loss: 0.941\n",
      "epoch: 3900, acc: 0.560, loss: 0.951\n",
      "epoch: 4000, acc: 0.543, loss: 0.952\n",
      "epoch: 4100, acc: 0.543, loss: 0.935\n",
      "epoch: 4200, acc: 0.517, loss: 0.933\n",
      "epoch: 4300, acc: 0.557, loss: 0.949\n",
      "epoch: 4400, acc: 0.537, loss: 0.934\n",
      "epoch: 4500, acc: 0.590, loss: 0.955\n",
      "epoch: 4600, acc: 0.590, loss: 0.967\n",
      "epoch: 4700, acc: 0.517, loss: 0.941\n",
      "epoch: 4800, acc: 0.510, loss: 0.964\n",
      "epoch: 4900, acc: 0.553, loss: 0.942\n",
      "epoch: 5000, acc: 0.510, loss: 0.932\n",
      "epoch: 5100, acc: 0.543, loss: 0.937\n",
      "epoch: 5200, acc: 0.587, loss: 0.952\n",
      "epoch: 5300, acc: 0.587, loss: 0.970\n",
      "epoch: 5400, acc: 0.543, loss: 0.941\n",
      "epoch: 5500, acc: 0.527, loss: 0.948\n",
      "epoch: 5600, acc: 0.547, loss: 0.917\n",
      "epoch: 5700, acc: 0.603, loss: 0.966\n",
      "epoch: 5800, acc: 0.553, loss: 0.944\n",
      "epoch: 5900, acc: 0.533, loss: 0.936\n",
      "epoch: 6000, acc: 0.577, loss: 0.970\n",
      "epoch: 6100, acc: 0.557, loss: 0.937\n",
      "epoch: 6200, acc: 0.603, loss: 0.959\n",
      "epoch: 6300, acc: 0.530, loss: 0.941\n",
      "epoch: 6400, acc: 0.577, loss: 0.936\n",
      "epoch: 6500, acc: 0.540, loss: 0.932\n",
      "epoch: 6600, acc: 0.590, loss: 0.961\n",
      "epoch: 6700, acc: 0.550, loss: 0.906\n",
      "epoch: 6800, acc: 0.570, loss: 0.950\n",
      "epoch: 6900, acc: 0.600, loss: 0.958\n",
      "epoch: 7000, acc: 0.523, loss: 0.926\n",
      "epoch: 7100, acc: 0.577, loss: 0.941\n",
      "epoch: 7200, acc: 0.550, loss: 0.921\n",
      "epoch: 7300, acc: 0.593, loss: 0.943\n",
      "epoch: 7400, acc: 0.593, loss: 0.940\n",
      "epoch: 7500, acc: 0.557, loss: 0.907\n",
      "epoch: 7600, acc: 0.590, loss: 0.949\n",
      "epoch: 7700, acc: 0.590, loss: 0.935\n",
      "epoch: 7800, acc: 0.563, loss: 0.932\n",
      "epoch: 7900, acc: 0.603, loss: 0.929\n",
      "epoch: 8000, acc: 0.557, loss: 0.913\n",
      "epoch: 8100, acc: 0.590, loss: 0.953\n",
      "epoch: 8200, acc: 0.587, loss: 0.894\n",
      "epoch: 8300, acc: 0.567, loss: 0.939\n",
      "epoch: 8400, acc: 0.580, loss: 0.910\n",
      "epoch: 8500, acc: 0.580, loss: 0.920\n",
      "epoch: 8600, acc: 0.587, loss: 0.887\n",
      "epoch: 8700, acc: 0.617, loss: 0.909\n",
      "epoch: 8800, acc: 0.623, loss: 0.853\n",
      "epoch: 8900, acc: 0.613, loss: 0.862\n",
      "epoch: 9000, acc: 0.623, loss: 0.864\n",
      "epoch: 9100, acc: 0.597, loss: 0.860\n",
      "epoch: 9200, acc: 0.630, loss: 0.842\n",
      "epoch: 9300, acc: 0.613, loss: 0.885\n",
      "epoch: 9400, acc: 0.603, loss: 0.893\n",
      "epoch: 9500, acc: 0.647, loss: 0.829\n",
      "epoch: 9600, acc: 0.623, loss: 0.836\n",
      "epoch: 9700, acc: 0.610, loss: 0.863\n",
      "epoch: 9800, acc: 0.633, loss: 0.848\n",
      "epoch: 9900, acc: 0.647, loss: 0.819\n",
      "epoch: 10000, acc: 0.657, loss: 0.816\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "nnfs.init()\n",
    "\n",
    "###NOTE: run full network code in final cell of this workbook so that this example works\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=.85)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100: #only every hundredth epoch\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate Decay\n",
    " - general idea is the start with some learning rate at beginning of training and decrease it during training\n",
    " - One way - monitor loss across each epoch and adjust learning rate if there is learning stagnation (ie. loss curve is flattening, rate too low) or jumping in loss (rate too high). Can be programmed or just done manually\n",
    " - Can also use a decay rate that decreases the learning rate per batch or per epoch, such as a 1/t decay or exponential decay.\n",
    " - we will use learning rate decay, specifically: starting_learning rate * (1 / (1+ learning_rate_decay * step_number)). So the larger the step number (scaled by the decay factor) the lower the learning rate. This form also ensure that we do not accidently increase learning rate by divinding by a value less than 1, hence the adding 1.\n",
    " - See below for new optimizer class with learning rate decay\n",
    " - decay is is defualt 0, and if decay is not 0, then we update it with the steps\n",
    " - we keep track of iterations, though not sure if it is necessary if epoch/batches are via range function. I guess it makes it more self-contained, and if you are not looping with numbers then it would be good to internally count iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning with Learning Rate\n",
    "- trial and error - learning rate decay of 1e-2 was too high, that is the rate dropped too fast and the model got stuck in local minimum. (higher decay causes larger denominator because we multiply decay by step size), 1e-3 is better, getting to our best accuracy/loss so far. This assumes learning rate of 1.\n",
    "- be sure to run optimzier object cell above and last cell in notebook with all prerequisite code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099\n",
      "epoch: 100, acc: 0.400, loss: 1.088\n",
      "epoch: 200, acc: 0.423, loss: 1.078\n",
      "epoch: 300, acc: 0.423, loss: 1.076\n",
      "epoch: 400, acc: 0.420, loss: 1.076\n",
      "epoch: 500, acc: 0.403, loss: 1.074\n",
      "epoch: 600, acc: 0.403, loss: 1.072\n",
      "epoch: 700, acc: 0.410, loss: 1.070\n",
      "epoch: 800, acc: 0.410, loss: 1.068\n",
      "epoch: 900, acc: 0.427, loss: 1.066\n",
      "epoch: 1000, acc: 0.440, loss: 1.063\n",
      "epoch: 1100, acc: 0.440, loss: 1.059\n",
      "epoch: 1200, acc: 0.447, loss: 1.056\n",
      "epoch: 1300, acc: 0.440, loss: 1.052\n",
      "epoch: 1400, acc: 0.427, loss: 1.048\n",
      "epoch: 1500, acc: 0.417, loss: 1.040\n",
      "epoch: 1600, acc: 0.423, loss: 1.033\n",
      "epoch: 1700, acc: 0.450, loss: 1.025\n",
      "epoch: 1800, acc: 0.470, loss: 1.017\n",
      "epoch: 1900, acc: 0.460, loss: 1.008\n",
      "epoch: 2000, acc: 0.463, loss: 1.000\n",
      "epoch: 2100, acc: 0.490, loss: 1.005\n",
      "epoch: 2200, acc: 0.467, loss: 1.014\n",
      "epoch: 2300, acc: 0.483, loss: 1.014\n",
      "epoch: 2400, acc: 0.490, loss: 1.012\n",
      "epoch: 2500, acc: 0.493, loss: 1.009\n",
      "epoch: 2600, acc: 0.497, loss: 1.005\n",
      "epoch: 2700, acc: 0.487, loss: 1.004\n",
      "epoch: 2800, acc: 0.483, loss: 0.999\n",
      "epoch: 2900, acc: 0.493, loss: 0.995\n",
      "epoch: 3000, acc: 0.490, loss: 0.991\n",
      "epoch: 3100, acc: 0.490, loss: 0.987\n",
      "epoch: 3200, acc: 0.493, loss: 0.983\n",
      "epoch: 3300, acc: 0.497, loss: 0.978\n",
      "epoch: 3400, acc: 0.493, loss: 0.974\n",
      "epoch: 3500, acc: 0.507, loss: 0.970\n",
      "epoch: 3600, acc: 0.530, loss: 0.966\n",
      "epoch: 3700, acc: 0.523, loss: 0.960\n",
      "epoch: 3800, acc: 0.527, loss: 0.956\n",
      "epoch: 3900, acc: 0.533, loss: 0.950\n",
      "epoch: 4000, acc: 0.533, loss: 0.946\n",
      "epoch: 4100, acc: 0.547, loss: 0.943\n",
      "epoch: 4200, acc: 0.550, loss: 0.937\n",
      "epoch: 4300, acc: 0.553, loss: 0.933\n",
      "epoch: 4400, acc: 0.557, loss: 0.929\n",
      "epoch: 4500, acc: 0.560, loss: 0.924\n",
      "epoch: 4600, acc: 0.567, loss: 0.919\n",
      "epoch: 4700, acc: 0.567, loss: 0.915\n",
      "epoch: 4800, acc: 0.577, loss: 0.911\n",
      "epoch: 4900, acc: 0.573, loss: 0.906\n",
      "epoch: 5000, acc: 0.577, loss: 0.900\n",
      "epoch: 5100, acc: 0.583, loss: 0.897\n",
      "epoch: 5200, acc: 0.587, loss: 0.892\n",
      "epoch: 5300, acc: 0.590, loss: 0.889\n",
      "epoch: 5400, acc: 0.593, loss: 0.886\n",
      "epoch: 5500, acc: 0.597, loss: 0.881\n",
      "epoch: 5600, acc: 0.617, loss: 0.876\n",
      "epoch: 5700, acc: 0.617, loss: 0.872\n",
      "epoch: 5800, acc: 0.623, loss: 0.868\n",
      "epoch: 5900, acc: 0.623, loss: 0.864\n",
      "epoch: 6000, acc: 0.633, loss: 0.860\n",
      "epoch: 6100, acc: 0.630, loss: 0.856\n",
      "epoch: 6200, acc: 0.640, loss: 0.852\n",
      "epoch: 6300, acc: 0.647, loss: 0.849\n",
      "epoch: 6400, acc: 0.653, loss: 0.845\n",
      "epoch: 6500, acc: 0.657, loss: 0.843\n",
      "epoch: 6600, acc: 0.640, loss: 0.839\n",
      "epoch: 6700, acc: 0.617, loss: 0.832\n",
      "epoch: 6800, acc: 0.657, loss: 0.829\n",
      "epoch: 6900, acc: 0.667, loss: 0.832\n",
      "epoch: 7000, acc: 0.663, loss: 0.827\n",
      "epoch: 7100, acc: 0.660, loss: 0.825\n",
      "epoch: 7200, acc: 0.673, loss: 0.821\n",
      "epoch: 7300, acc: 0.657, loss: 0.817\n",
      "epoch: 7400, acc: 0.667, loss: 0.814\n",
      "epoch: 7500, acc: 0.657, loss: 0.811\n",
      "epoch: 7600, acc: 0.653, loss: 0.808\n",
      "epoch: 7700, acc: 0.647, loss: 0.806\n",
      "epoch: 7800, acc: 0.647, loss: 0.804\n",
      "epoch: 7900, acc: 0.643, loss: 0.802\n",
      "epoch: 8000, acc: 0.647, loss: 0.799\n",
      "epoch: 8100, acc: 0.647, loss: 0.797\n",
      "epoch: 8200, acc: 0.640, loss: 0.796\n",
      "epoch: 8300, acc: 0.643, loss: 0.794\n",
      "epoch: 8400, acc: 0.643, loss: 0.793\n",
      "epoch: 8500, acc: 0.637, loss: 0.791\n",
      "epoch: 8600, acc: 0.640, loss: 0.790\n",
      "epoch: 8700, acc: 0.640, loss: 0.788\n",
      "epoch: 8800, acc: 0.643, loss: 0.787\n",
      "epoch: 8900, acc: 0.643, loss: 0.785\n",
      "epoch: 9000, acc: 0.647, loss: 0.784\n",
      "epoch: 9100, acc: 0.647, loss: 0.782\n",
      "epoch: 9200, acc: 0.647, loss: 0.781\n",
      "epoch: 9300, acc: 0.643, loss: 0.780\n",
      "epoch: 9400, acc: 0.650, loss: 0.778\n",
      "epoch: 9500, acc: 0.653, loss: 0.777\n",
      "epoch: 9600, acc: 0.657, loss: 0.776\n",
      "epoch: 9700, acc: 0.657, loss: 0.774\n",
      "epoch: 9800, acc: 0.663, loss: 0.773\n",
      "epoch: 9900, acc: 0.663, loss: 0.772\n",
      "epoch: 10000, acc: 0.667, loss: 0.771\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "nnfs.init()\n",
    "\n",
    "###NOTE: run full network code in final cell of this workbook so that this example works\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100: #only every hundredth epoch\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum\n",
    "- momentum creates a rolling average of gradients over some number of epochs or training steps\n",
    "- helps to avoid getting stuck in local minimums because if previous gradients were very high, they will play into the current weight update, which might be small because the model is near a local minimum. So the momentum will increase the weight update preventing the model from getting stuck in the local minimum\n",
    "- kind of like a ball rolling down a hill, the steeper the hill, the bigger the hill on the other side you need to slow it down\n",
    "- in other words, the momentum helps you stay on the path of global gradient descent -looking at the greater overall average gradient - than just the gradient that is what is right next to you.\n",
    "- weight_updates_with_momentum = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights; where self.momentum is a momentum hyperparameter between 0 and 1 and weight momentums is an array of previous weight momentums, consisting of the previous weight updates: layer.weight_momentums = weight_updates, instantiated at 0s.\n",
    "- so basically the momentum is the previous update to the parameters, then we just incorporate new gradient info by subtracting the current gradient times the learning rate. The greater the momentum parameter, the greater the weight on the previous gradients * learning rates\n",
    "- thinking through, if momentum parameter is > 0 and the previous step was a very high gradient, then the next step is in the same direction, then the gradient update will increase. If it is in the opposite then it will decrease, but be greater than it would've been without momentum. This is where the momentum prevents the falling into local minimum. With each new step, there is greater emphasis on more recent gradient updates and the other update values are overwritten by the rolling sum, and more so if momentum param < 1. This is an exponential moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Update Paras function for Optimizer SGD\n",
    "- checks if momentum is not 0, if not\n",
    "- check if we have the attribute weight momentums (ie, is this the first epoch/step), and if not, then makes a gradient of 0s\n",
    "- performs the formula described above, and then sets the weight momentums to that value for the next epoch when the new weights are calculated.\n",
    "- then, updates weights like done previously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(self, layer):\n",
    "# If we use momentum\n",
    "    if self.momentum:\n",
    "    # If layer does not contain momentum arrays, create them\n",
    "    # filled with zeros\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            # If there is no momentum array for weights\n",
    "            # The array doesn't exist for biases yet either.\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Build weight updates with momentum - take previous\n",
    "        # updates multiplied by retain factor and update with\n",
    "        # current gradients\n",
    "        weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "        layer.weight_momentums = weight_updates\n",
    "        # Build bias updates\n",
    "        bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "        layer.bias_momentums = bias_updates\n",
    "\n",
    "    else:\n",
    "        weight_updates = -self.current_learning_rate * layer.dweights\n",
    "        bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    layer.weights += weight_updates\n",
    "    layer.biases += bias_updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Optimizer With Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "    # If we use momentum\n",
    "        if self.momentum:\n",
    "        # If layer does not contain momentum arrays, create them\n",
    "        # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Momentum\n",
    "- be sure to run cell above for optimizer class with momentum AND run the last cell with the other network class\n",
    "- performance with momentum =.9 is the best we've seen so far => accuracy 93.3% and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.443, loss: 1.053, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.497, loss: 0.999, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.603, loss: 0.810, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.700, loss: 0.700, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.750, loss: 0.595, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.810, loss: 0.496, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.810, loss: 0.466, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.847, loss: 0.384, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.850, loss: 0.364, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.877, loss: 0.344, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.863, loss: 0.347, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.880, loss: 0.322, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.887, loss: 0.309, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.877, loss: 0.295, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.887, loss: 0.277, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.893, loss: 0.270, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.890, loss: 0.265, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.890, loss: 0.260, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.893, loss: 0.255, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.900, loss: 0.250, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.897, loss: 0.246, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.900, loss: 0.242, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.903, loss: 0.238, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.903, loss: 0.234, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.903, loss: 0.230, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.900, loss: 0.226, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.903, loss: 0.222, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.900, loss: 0.219, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.910, loss: 0.216, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.910, loss: 0.214, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.910, loss: 0.212, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.910, loss: 0.211, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.910, loss: 0.209, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.913, loss: 0.208, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.913, loss: 0.206, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.917, loss: 0.205, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.917, loss: 0.204, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.920, loss: 0.202, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.920, loss: 0.201, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.920, loss: 0.201, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.920, loss: 0.200, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.920, loss: 0.199, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.923, loss: 0.198, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.923, loss: 0.197, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.923, loss: 0.197, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.923, loss: 0.196, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.920, loss: 0.195, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.923, loss: 0.195, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.923, loss: 0.194, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.920, loss: 0.193, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.923, loss: 0.192, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.920, loss: 0.191, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.920, loss: 0.190, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.920, loss: 0.190, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.920, loss: 0.189, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.923, loss: 0.188, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.923, loss: 0.188, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.923, loss: 0.187, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.923, loss: 0.187, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.923, loss: 0.186, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.927, loss: 0.186, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.927, loss: 0.185, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.927, loss: 0.185, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.927, loss: 0.184, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.923, loss: 0.184, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.927, loss: 0.184, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.927, loss: 0.183, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.927, loss: 0.183, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.930, loss: 0.182, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.927, loss: 0.182, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.930, loss: 0.181, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.930, loss: 0.181, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.930, loss: 0.180, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.933, loss: 0.180, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.930, loss: 0.180, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.933, loss: 0.179, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.937, loss: 0.179, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.930, loss: 0.179, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.933, loss: 0.178, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.933, loss: 0.178, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.933, loss: 0.178, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.930, loss: 0.177, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.930, loss: 0.177, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.933, loss: 0.177, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.933, loss: 0.177, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.933, loss: 0.176, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.933, loss: 0.175, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.933, loss: 0.175, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.930, loss: 0.175, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.933, loss: 0.175, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.933, loss: 0.175, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.933, loss: 0.174, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.930, loss: 0.174, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.933, loss: 0.174, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.933, loss: 0.174, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.933, loss: 0.174, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.933, loss: 0.173, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.933, loss: 0.173, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.933, loss: 0.173, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.933, loss: 0.173, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "nnfs.init()\n",
    "\n",
    "###NOTE: run full network code in final cell of this workbook so that this example works\n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(decay=1e-3, momentum=.9)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    \n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100: #only every hundredth epoch\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, ' +\n",
    "        f'loss: {loss:.3f}, ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Network Code from Ch. 9 So We can make use of it for examples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # lets make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # Return loss\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "# Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "    # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # For each row in dinputs, get what the network has for the correct class and subtract 1\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
