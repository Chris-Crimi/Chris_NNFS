{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout\n",
    "- this is another method of network regularization\n",
    "- involves turning on and off some neurons in a layer in the network, called dropout layer, during training. This allows some inputs to the layer to pass through, while stopping others. Neurons to be turned off are randomly selected, with a given probability during each forward pass (i.e. 30% or 50% of neurons are turned off in a forward pass, neurons selected at random for each pass)\n",
    "- forces the model to make use of all its neurons. This can help with:\n",
    "    1) Prevents the network from becoming to reliant on any one neuron or reliance on a single neuron for a single instance.\n",
    "    2) Preventing overfitting/memorizing the data as model cannot rely on specific neurons to memorize samples \n",
    "    3) Preventing co-adaptation - which is when a neuron becomes highly dependent on the outputs of another pre-synaptic neuron. Instead of learning the underlying function, it becomes specialized to the output of one of its input neurons. This can lead to poor generalization and overfitting\n",
    "    4) Helpful if training data is noisy or has other perturbations. Dropout is its own form of noise - thus helping the network better generalize to noisy data. Exposing network to different subset of neurons during training teaches it to be more resilient to noise and variations in input data\n",
    "- despite \"turning off\" some neurons, this does not speed up training process, as the dropped-out are not truly turned off, just their outputs are zeroed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Pass\n",
    "- firstly - bernoulli distribution and binomial distribtion. \n",
    "    1) A bernoulli distribution is the probability of a binary outcome for a single trial, like a coin toss, where outcomes are mutually exclusive. There is either heads or tails. In this scenario, heads =1, tails = 0. Probability of heads,p = .5; probability of tails = 1 - p. Generally p can anything between 0 and 1.\n",
    "    2) a binomial distribtion is like the general case of a bernoulli distribtion. In general form, we can do multiple trials for outcomes from the distribution, summing them. So bernoulli distribution is a special case of binomial distribtion where a single trial is conducted. For example if we have a binomial distribtion with number of trials = 2, and p > 0. The possible range of values is 0, 1 , and 2 from this binomial distribtion. This is because when (p>0) and it is possible, in two trials, to draw a 1 each time. So summing these up, the binomial distribtion outcome is 0. It is 0 for the same reason, if p<1. and 1 may occur by drawing 1/0 or 0/1 on the trials. In the example given, (i.e. 1 or 0, where p = .5, n_samples = 2) there is 50% of a 1 (.5 + .5), and 25% of 0 or 2 (.5 * .5).\n",
    "- to create our dropout, we will use a binomial distribution with n_samples = 1, and p = share of neurons we want to drop out. In some network frameworks, p may be defined oppositely, where it is the share of neurons being kept. It is just important to be aware of which way it is. We can use numpy to create an array with individual trials in each value with our desired binomial setting. See the example 1 code below - the function creates an array which should show at least one 0 out of the five positions in the array, but this will not always be the case as it is probabilistic. So on average, there will be four 1s and one 0. See example 2, it is the basic method by which dropout will be implemented.\n",
    "- just dropping half the neurons presents an issue because we will be cutting the output by approximately the share of neurons that we drop. So similar to the normalization for adam, we need to divide by the (1 - dropout_rate) to rescale the dropout outputs back to their full values. So this way, we don't have to account for the dropout during prediction, and with enough samples, this scaling should average out to the approximate amount it would be without dropout. See example 3, notice how after scaling across many samples, the dropout sum is very close to the actual sum without dropout (intial sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 output:  [1 1 1 1 1]\n",
      "Example 2 output:  [ 0.   -0.    0.    0.99  0.05 -0.   -0.    1.13 -0.07  0.73]\n",
      "Example 3 output\n",
      "sum initial 0.36000000000000015\n",
      "mean sum: 0.3605887500000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "##Example 1\n",
    "dropout_rate = 0.20 #how many should be 0\n",
    "print(\"Example 1 output: \",np.random.binomial(1, 1-dropout_rate, size=5))\n",
    "\n",
    "##Example 2\n",
    "import numpy as np\n",
    "dropout_rate = 0.3\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "example_output *= np.random.binomial(1, 1-dropout_rate, example_output.shape) #final argument is called size in the function, but we need to pass shape. It's confusing.\n",
    "print(\"Example 2 output: \",example_output)\n",
    "\n",
    "##Example 3\n",
    "print(\"Example 3 output\")\n",
    "import numpy as np\n",
    "dropout_rate = 0.2\n",
    "example_output = np.array([0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73])\n",
    "print(f'sum initial {sum(example_output)}')\n",
    "sums = []\n",
    "for i in range(10000):\n",
    "    example_output2 = example_output * np.random.binomial(1, 1-dropout_rate, example_output.shape) / (1-dropout_rate)\n",
    "    sums.append(sum(example_output2))\n",
    "print(f'mean sum: {np.mean(sums)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Pass\n",
    "- We have done two changes to the dropout layer that must be accounted for\n",
    "    1) as described, during dropout, a neuron can have two states, dropped or not dropped, so the neuron's output is either the output or 0\n",
    "    2) then, we have also normalized this output so that it is closer to the true sum (see discussion above).\n",
    "\n",
    "- we'll start with the scalar derivative (dividing by (1- dropout)). At first glance it may seem like quotient rule, but actually we just have a linear equation: f(x) = 1/(1 - q)*x => f'(x) = 1/(1-q), where 1 = dropout rate. We just treat the 1/(1-q) as a constant, so the derivative becomes simple dropping the linear variable. Note, that we use this derivative for neurons that are not dropped in the dropout layer.\n",
    "- for neurons that are dropped, the derivative is 0. Intuitively, this makes sense, they are not contributing at all to loss.\n",
    "- Please note that the dropout itself does not have any neurons, it just goes after a given layer in the network to turn off some neurons. So that's why the derivative is 1/(1-q) for not dropped neurons => their weight/input/bias derivatives are handled in layer before the dropout layer (from a forward pass perspective).\n",
    "- full dropout derivative: where droput is D(z) and ri is the 1/0 mask for dropping neurons, z is the input from previous layer, q is dropout rate; \n",
    "    - D(z) = z/(1-q) where ri=1, 0 where ri=0\n",
    "    - D'(z) = 1/(1-q) where ri=1, 0 where ri=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Dropout Layer\n",
    "- the binary mask variable is the same as the derivative of the layer, so instead of writing code for the derivative, we take the binary mask used in the forward pass and just re-use in on the backward pass. That is why it is just dvalues (the values passed in from previous layer multiplied by binary mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "        # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "        size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout performance:\n",
    "- run the final cell in the workbook for this cell to work\n",
    "- with 64 neuron sized layers, the network has 68.8% testing accuracy and 75.7% validation accuracy. So validation performs better becuase the network is able to use all its neurons. Typically validation accuracy is higher than training with dropout for this reason.\n",
    "- greater network size (not shown), with 512 per layer, still performs worse on validation than no dropout version, and training accuracy is slightly higher than validation, so one might suspect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.324, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), lr: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, acc: 0.545, loss: 0.923 (data_loss: 0.900, reg_loss: 0.023), lr: 0.04975371909050202\n",
      "epoch: 200, acc: 0.584, loss: 0.891 (data_loss: 0.863, reg_loss: 0.028), lr: 0.049507401356502806\n",
      "epoch: 300, acc: 0.609, loss: 0.871 (data_loss: 0.844, reg_loss: 0.027), lr: 0.0492635105177595\n",
      "epoch: 400, acc: 0.611, loss: 0.849 (data_loss: 0.823, reg_loss: 0.026), lr: 0.04902201088288642\n",
      "epoch: 500, acc: 0.615, loss: 0.847 (data_loss: 0.822, reg_loss: 0.025), lr: 0.048782867456949125\n",
      "epoch: 600, acc: 0.632, loss: 0.839 (data_loss: 0.815, reg_loss: 0.024), lr: 0.04854604592455945\n",
      "epoch: 700, acc: 0.604, loss: 0.830 (data_loss: 0.804, reg_loss: 0.026), lr: 0.048311512633460556\n",
      "epoch: 800, acc: 0.652, loss: 0.823 (data_loss: 0.797, reg_loss: 0.026), lr: 0.04807923457858551\n",
      "epoch: 900, acc: 0.631, loss: 0.804 (data_loss: 0.778, reg_loss: 0.025), lr: 0.04784917938657352\n",
      "epoch: 1000, acc: 0.656, loss: 0.801 (data_loss: 0.776, reg_loss: 0.026), lr: 0.04762131530072861\n",
      "epoch: 1100, acc: 0.628, loss: 0.806 (data_loss: 0.780, reg_loss: 0.026), lr: 0.04739561116640599\n",
      "epoch: 1200, acc: 0.658, loss: 0.783 (data_loss: 0.757, reg_loss: 0.026), lr: 0.04717203641681212\n",
      "epoch: 1300, acc: 0.643, loss: 0.788 (data_loss: 0.762, reg_loss: 0.026), lr: 0.04695056105920466\n",
      "epoch: 1400, acc: 0.660, loss: 0.784 (data_loss: 0.758, reg_loss: 0.026), lr: 0.04673115566147951\n",
      "epoch: 1500, acc: 0.670, loss: 0.788 (data_loss: 0.761, reg_loss: 0.026), lr: 0.046513791339132055\n",
      "epoch: 1600, acc: 0.678, loss: 0.778 (data_loss: 0.752, reg_loss: 0.026), lr: 0.04629843974258068\n",
      "epoch: 1700, acc: 0.651, loss: 0.777 (data_loss: 0.751, reg_loss: 0.025), lr: 0.046085073044840774\n",
      "epoch: 1800, acc: 0.661, loss: 0.770 (data_loss: 0.745, reg_loss: 0.025), lr: 0.04587366392953806\n",
      "epoch: 1900, acc: 0.662, loss: 0.789 (data_loss: 0.764, reg_loss: 0.025), lr: 0.04566418557925019\n",
      "epoch: 2000, acc: 0.667, loss: 0.780 (data_loss: 0.756, reg_loss: 0.024), lr: 0.045456611664166556\n",
      "epoch: 2100, acc: 0.647, loss: 0.774 (data_loss: 0.750, reg_loss: 0.024), lr: 0.045250916331055706\n",
      "epoch: 2200, acc: 0.644, loss: 0.763 (data_loss: 0.740, reg_loss: 0.024), lr: 0.0450470741925312\n",
      "epoch: 2300, acc: 0.657, loss: 0.779 (data_loss: 0.756, reg_loss: 0.024), lr: 0.04484506031660612\n",
      "epoch: 2400, acc: 0.665, loss: 0.756 (data_loss: 0.732, reg_loss: 0.024), lr: 0.04464485021652753\n",
      "epoch: 2500, acc: 0.658, loss: 0.762 (data_loss: 0.738, reg_loss: 0.024), lr: 0.044446419840881816\n",
      "epoch: 2600, acc: 0.674, loss: 0.749 (data_loss: 0.725, reg_loss: 0.024), lr: 0.04424974556396301\n",
      "epoch: 2700, acc: 0.666, loss: 0.763 (data_loss: 0.740, reg_loss: 0.024), lr: 0.04405480417639544\n",
      "epoch: 2800, acc: 0.690, loss: 0.747 (data_loss: 0.724, reg_loss: 0.023), lr: 0.04386157287600334\n",
      "epoch: 2900, acc: 0.678, loss: 0.741 (data_loss: 0.718, reg_loss: 0.023), lr: 0.04367002925891961\n",
      "epoch: 3000, acc: 0.672, loss: 0.759 (data_loss: 0.737, reg_loss: 0.023), lr: 0.043480151310926564\n",
      "epoch: 3100, acc: 0.679, loss: 0.747 (data_loss: 0.724, reg_loss: 0.023), lr: 0.04329191739902161\n",
      "epoch: 3200, acc: 0.671, loss: 0.742 (data_loss: 0.719, reg_loss: 0.023), lr: 0.043105306263201\n",
      "epoch: 3300, acc: 0.674, loss: 0.753 (data_loss: 0.730, reg_loss: 0.022), lr: 0.0429202970084553\n",
      "epoch: 3400, acc: 0.665, loss: 0.758 (data_loss: 0.736, reg_loss: 0.022), lr: 0.04273686909696996\n",
      "epoch: 3500, acc: 0.670, loss: 0.740 (data_loss: 0.719, reg_loss: 0.022), lr: 0.04255500234052514\n",
      "epoch: 3600, acc: 0.664, loss: 0.728 (data_loss: 0.707, reg_loss: 0.022), lr: 0.042374676893088686\n",
      "epoch: 3700, acc: 0.687, loss: 0.737 (data_loss: 0.715, reg_loss: 0.021), lr: 0.042195873243596776\n",
      "epoch: 3800, acc: 0.686, loss: 0.744 (data_loss: 0.723, reg_loss: 0.021), lr: 0.04201857220891634\n",
      "epoch: 3900, acc: 0.684, loss: 0.743 (data_loss: 0.723, reg_loss: 0.021), lr: 0.041842754926984395\n",
      "epoch: 4000, acc: 0.669, loss: 0.732 (data_loss: 0.711, reg_loss: 0.021), lr: 0.04166840285011875\n",
      "epoch: 4100, acc: 0.688, loss: 0.738 (data_loss: 0.717, reg_loss: 0.020), lr: 0.041495497738495375\n",
      "epoch: 4200, acc: 0.665, loss: 0.748 (data_loss: 0.727, reg_loss: 0.021), lr: 0.041324021653787346\n",
      "epoch: 4300, acc: 0.668, loss: 0.742 (data_loss: 0.721, reg_loss: 0.021), lr: 0.041153956952961035\n",
      "epoch: 4400, acc: 0.687, loss: 0.737 (data_loss: 0.717, reg_loss: 0.021), lr: 0.040985286282224684\n",
      "epoch: 4500, acc: 0.679, loss: 0.740 (data_loss: 0.719, reg_loss: 0.021), lr: 0.04081799257112535\n",
      "epoch: 4600, acc: 0.678, loss: 0.744 (data_loss: 0.723, reg_loss: 0.021), lr: 0.04065205902678971\n",
      "epoch: 4700, acc: 0.683, loss: 0.731 (data_loss: 0.711, reg_loss: 0.020), lr: 0.04048746912830479\n",
      "epoch: 4800, acc: 0.672, loss: 0.744 (data_loss: 0.724, reg_loss: 0.020), lr: 0.04032420662123473\n",
      "epoch: 4900, acc: 0.681, loss: 0.736 (data_loss: 0.716, reg_loss: 0.020), lr: 0.04016225551226957\n",
      "epoch: 5000, acc: 0.684, loss: 0.710 (data_loss: 0.690, reg_loss: 0.020), lr: 0.04000160006400256\n",
      "epoch: 5100, acc: 0.667, loss: 0.758 (data_loss: 0.738, reg_loss: 0.020), lr: 0.039842224789832265\n",
      "epoch: 5200, acc: 0.685, loss: 0.729 (data_loss: 0.709, reg_loss: 0.020), lr: 0.03968411444898608\n",
      "epoch: 5300, acc: 0.681, loss: 0.733 (data_loss: 0.713, reg_loss: 0.019), lr: 0.03952725404166173\n",
      "epoch: 5400, acc: 0.675, loss: 0.736 (data_loss: 0.717, reg_loss: 0.019), lr: 0.03937162880428363\n",
      "epoch: 5500, acc: 0.675, loss: 0.719 (data_loss: 0.700, reg_loss: 0.019), lr: 0.03921722420487078\n",
      "epoch: 5600, acc: 0.673, loss: 0.746 (data_loss: 0.727, reg_loss: 0.019), lr: 0.03906402593851323\n",
      "epoch: 5700, acc: 0.675, loss: 0.728 (data_loss: 0.709, reg_loss: 0.019), lr: 0.038912019922954205\n",
      "epoch: 5800, acc: 0.693, loss: 0.737 (data_loss: 0.718, reg_loss: 0.019), lr: 0.038761192294274965\n",
      "epoch: 5900, acc: 0.683, loss: 0.734 (data_loss: 0.716, reg_loss: 0.018), lr: 0.038611529402679645\n",
      "epoch: 6000, acc: 0.678, loss: 0.729 (data_loss: 0.711, reg_loss: 0.018), lr: 0.03846301780837725\n",
      "epoch: 6100, acc: 0.683, loss: 0.730 (data_loss: 0.712, reg_loss: 0.018), lr: 0.03831564427755853\n",
      "epoch: 6200, acc: 0.683, loss: 0.733 (data_loss: 0.715, reg_loss: 0.018), lr: 0.03816939577846483\n",
      "epoch: 6300, acc: 0.691, loss: 0.741 (data_loss: 0.723, reg_loss: 0.018), lr: 0.038024259477546674\n",
      "epoch: 6400, acc: 0.654, loss: 0.739 (data_loss: 0.721, reg_loss: 0.018), lr: 0.03788022273570969\n",
      "epoch: 6500, acc: 0.695, loss: 0.735 (data_loss: 0.717, reg_loss: 0.018), lr: 0.03773727310464546\n",
      "epoch: 6600, acc: 0.666, loss: 0.753 (data_loss: 0.735, reg_loss: 0.018), lr: 0.03759539832324524\n",
      "epoch: 6700, acc: 0.655, loss: 0.743 (data_loss: 0.725, reg_loss: 0.018), lr: 0.03745458631409416\n",
      "epoch: 6800, acc: 0.696, loss: 0.731 (data_loss: 0.713, reg_loss: 0.018), lr: 0.03731482518004403\n",
      "epoch: 6900, acc: 0.682, loss: 0.730 (data_loss: 0.713, reg_loss: 0.018), lr: 0.03717610320086248\n",
      "epoch: 7000, acc: 0.677, loss: 0.751 (data_loss: 0.734, reg_loss: 0.018), lr: 0.03703840882995667\n",
      "epoch: 7100, acc: 0.681, loss: 0.745 (data_loss: 0.727, reg_loss: 0.017), lr: 0.036901730691169414\n",
      "epoch: 7200, acc: 0.680, loss: 0.727 (data_loss: 0.710, reg_loss: 0.017), lr: 0.03676605757564617\n",
      "epoch: 7300, acc: 0.663, loss: 0.732 (data_loss: 0.715, reg_loss: 0.017), lr: 0.03663137843877066\n",
      "epoch: 7400, acc: 0.681, loss: 0.732 (data_loss: 0.715, reg_loss: 0.017), lr: 0.03649768239716778\n",
      "epoch: 7500, acc: 0.657, loss: 0.745 (data_loss: 0.728, reg_loss: 0.017), lr: 0.03636495872577185\n",
      "epoch: 7600, acc: 0.682, loss: 0.727 (data_loss: 0.710, reg_loss: 0.017), lr: 0.03623319685495851\n",
      "epoch: 7700, acc: 0.685, loss: 0.722 (data_loss: 0.705, reg_loss: 0.017), lr: 0.03610238636773891\n",
      "epoch: 7800, acc: 0.673, loss: 0.733 (data_loss: 0.716, reg_loss: 0.017), lr: 0.03597251699701428\n",
      "epoch: 7900, acc: 0.683, loss: 0.738 (data_loss: 0.721, reg_loss: 0.017), lr: 0.035843578622889706\n",
      "epoch: 8000, acc: 0.685, loss: 0.727 (data_loss: 0.710, reg_loss: 0.017), lr: 0.03571556127004536\n",
      "epoch: 8100, acc: 0.684, loss: 0.716 (data_loss: 0.699, reg_loss: 0.017), lr: 0.03558845510516389\n",
      "epoch: 8200, acc: 0.689, loss: 0.721 (data_loss: 0.704, reg_loss: 0.017), lr: 0.03546225043441257\n",
      "epoch: 8300, acc: 0.699, loss: 0.708 (data_loss: 0.692, reg_loss: 0.016), lr: 0.035336937700978836\n",
      "epoch: 8400, acc: 0.672, loss: 0.719 (data_loss: 0.703, reg_loss: 0.016), lr: 0.03521250748265784\n",
      "epoch: 8500, acc: 0.686, loss: 0.702 (data_loss: 0.686, reg_loss: 0.016), lr: 0.035088950489490865\n",
      "epoch: 8600, acc: 0.690, loss: 0.726 (data_loss: 0.710, reg_loss: 0.016), lr: 0.0349662575614532\n",
      "epoch: 8700, acc: 0.689, loss: 0.707 (data_loss: 0.691, reg_loss: 0.016), lr: 0.034844419666190465\n",
      "epoch: 8800, acc: 0.671, loss: 0.729 (data_loss: 0.713, reg_loss: 0.016), lr: 0.034723427896801974\n",
      "epoch: 8900, acc: 0.695, loss: 0.713 (data_loss: 0.696, reg_loss: 0.016), lr: 0.03460327346967023\n",
      "epoch: 9000, acc: 0.680, loss: 0.727 (data_loss: 0.711, reg_loss: 0.016), lr: 0.034483947722335255\n",
      "epoch: 9100, acc: 0.693, loss: 0.735 (data_loss: 0.719, reg_loss: 0.016), lr: 0.034365442111412764\n",
      "epoch: 9200, acc: 0.696, loss: 0.742 (data_loss: 0.726, reg_loss: 0.016), lr: 0.03424774821055516\n",
      "epoch: 9300, acc: 0.687, loss: 0.716 (data_loss: 0.700, reg_loss: 0.016), lr: 0.03413085770845422\n",
      "epoch: 9400, acc: 0.688, loss: 0.732 (data_loss: 0.716, reg_loss: 0.016), lr: 0.034014762406884586\n",
      "epoch: 9500, acc: 0.690, loss: 0.708 (data_loss: 0.692, reg_loss: 0.016), lr: 0.03389945421878708\n",
      "epoch: 9600, acc: 0.694, loss: 0.731 (data_loss: 0.716, reg_loss: 0.016), lr: 0.033784925166390756\n",
      "epoch: 9700, acc: 0.686, loss: 0.749 (data_loss: 0.733, reg_loss: 0.016), lr: 0.03367116737937304\n",
      "epoch: 9800, acc: 0.688, loss: 0.731 (data_loss: 0.715, reg_loss: 0.016), lr: 0.033558173093056816\n",
      "epoch: 9900, acc: 0.668, loss: 0.733 (data_loss: 0.717, reg_loss: 0.016), lr: 0.0334459346466437\n",
      "epoch: 10000, acc: 0.688, loss: 0.727 (data_loss: 0.711, reg_loss: 0.016), lr: 0.03333444448148271\n",
      "validation, acc: 0.757, loss: 0.712\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=1000, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "bias_regularizer_l2=5e-4)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create dropout layer\n",
    "dropout1 = Layer_Dropout(0.1)\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "# Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through Dropout layer\n",
    "    dropout1.forward(activation1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(dropout1.output)\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    \n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f} (' + f'data_loss: {data_loss:.3f}, ' + f'reg_loss: {regularization_loss:.3f}), ' + f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate the model\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full network code thru this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, \n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "        # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "        size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let’s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "# Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "    # 0 by default\n",
    "        regularization_loss = 0\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        return regularization_loss\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "        \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "# Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "    # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # For each row in dinputs, get what the network has for the correct class and subtract 1\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "    # If we use momentum\n",
    "        if self.momentum:\n",
    "        # If layer does not contain momentum arrays, create them\n",
    "        # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=.001, decay=0., epsilon=1e-7, rho=.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "    # If layer does not contain cache arrays,\n",
    "    # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
