{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Network Error with Loss\n",
    "- loss function = cost function\n",
    "- loss calculates how wrong a network is from the correct answer and is the model's error. Thus, ideally loss should be zero.\n",
    "- Classification network outputs are akin to the confidence of the network's classification, and thus want to increase confidence (i.e., move correct neuron closer to 1) and decrease misplaced confidence\n",
    "- For the current task at hand, we will use categorical cross entropy loss, however for differeny types of network outputs, there are obviously different functions => Mean squared error (regression), Binary Cross entropy loss (sigmoid activation function w/ two mutually exclusive classes and single output neuron; aka log loss?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Cross Entropy loss (Note: did some extra reading outside the book)\n",
    "- used for multiple mutually exclusive classes in classification task, thus commonly used with a softmax activation layer\n",
    "- cross entropy means the differnce between two distributions, in our case, the output distribution of the network and the actual ground truth distribution.\n",
    "- categorical comes from the fact that the ground truth distribution is category based (i.e., there is only one correct category and not varying degrees of correctness/probability e.g., one hot encoded or sparse)\n",
    "- categorical cross entropy loss = -sum((ground_truth_value(i) * -log(predicted_value(i))); where i is the ith value in the softmax output matrix and the ground truth matrix is one hot encoded\n",
    "- one hot encoded is an array/matrix where the correct value or desired values are 1 and the rest are 0\n",
    "- so the above equation in categorical cross entropy loss results in all the wrong classes being multiplied by 0 and the correct class being multiplied by 1. This results in simplification in code to just -log(predicted_value_of_correct_class)\n",
    "- going forward references to log in the book mean natural log (ln = log with base e)\n",
    "- Ultimately => goal is to calculate average categorical cross entropy loss for each training batch\n",
    "\n",
    "Further math intuition => \n",
    "- -log(x) is downward sloping and where x = 1, -log(x) = 0; which works because if your network is predicting 1 for the correct class, then the loss is 0. => 1 * -log(0) = 0;\n",
    "- As confidence decreases (lower output value), the loss approaches infinity, there is an asymptote at x = 0; this will present a problem later in the book (need to add very small value to predicted probability so not passing 0) bc log(0) = undefined)\n",
    "\n",
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# Ground truth\n",
    "\n",
    "target_output = [1, 0, 0]\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "math.log(softmax_output[1])*target_output[1] +\n",
    "math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)\n",
    "\n",
    "#simplification - see notes above for explanation => don't need to include other terms besides one in desired \n",
    "# ground truth because they go to 0\n",
    "\n",
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamically Taking the Log of Desired Index Point \n",
    "- have layer output and the correct answer for the layer in an array and list\n",
    "- this list can be one-hot-encoded (explained and exemplified above), or sparse (below)\n",
    "- sparse means that ground-truth array contains numbers representing the correct classes, such as 0 = dog, 1 = cat, 2 = human. So [1, 0 , 2], would correspond with 3 feature samples, whose ground trought outputs are cat, dog, human. As opposed to one-hot-encoded where cat would be [0, 1, 0]. So a sparse array will be single dimension, whereas one hot encoded will be multi dimensonal\n",
    "- in the below examples, the loss is averaged, this also applies to one-hot, just did not show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n",
      "[0.7 0.5 0.9]\n",
      "[0.7 0.5 0.9]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "#where the value represents a class; e.g., 0 = dog, 1 = cat, 2 = human; so dog, cat, cat here\n",
    "class_targets = [0, 1, 1] #sparse encoding\n",
    "\n",
    "#one way to get this\n",
    "\n",
    "#for each row in the outputs, get the value in that row corresponding with the correct class, aka for each row, get column\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])\n",
    "\n",
    "#even faster using numpy => get the [[row_numbers, col_number]], getting each row here because we want each output\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])\n",
    "\n",
    "#so since we want to get the target value at each row, we always want to get each row, so can make further dynamic\n",
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "#range len counts off each row for the length of the softmax outputs array\n",
    "\n",
    "##full sparse simplification and log and average of loss => averaging applies to one-hot too\n",
    "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling One hot and sparse ground truth encodings\n",
    "- to make network as flexible as possible and be able to handle multiple ground truth formats (one-hot and sparse), are implementing the code below\n",
    "- can test whether the ground truth array is one hot or sparse by looking at dimensions; 2D array is one hot because each output row is a list of 1s and 0s for the hot and cold classes of the respective feature sample. Sparse is 1D because each value in array communicates the ground truth class for its respective feature sample, which is also designated to same the column (neuron) location index in the output array (see examples above), so if class is 0 then output position is the first spot in the row, so on an so forth \n",
    "- implementation: np arrays have property variable shape, which describes their dimensionality. If shape is tuple length of 1, then shape is 1D, if tuple length of 2 then shape is 2D, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "\n",
    "#implementation see notes above\n",
    "\n",
    "if len(class_targets.shape) == 1: #if 1D\n",
    "    correct_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "elif len(class_targets.shape) == 2: #if 2D\n",
    "    correct_confidences = np.sum(softmax_outputs*class_targets, axis = 1) #axis = 1 to sum the values of each row\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "\n",
    "print(average_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
