{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Network Error with Loss\n",
    "- loss function = cost function\n",
    "- loss calculates how wrong a network is from the correct answer and is the model's error. Thus, ideally loss should be zero.\n",
    "- Classification network outputs are akin to the confidence of the network's classification, and thus want to increase confidence (i.e., move correct neuron closer to 1) and decrease misplaced confidence\n",
    "- For the current task at hand, we will use categorical cross entropy loss, however for differeny types of network outputs, there are obviously different functions => Mean squared error (regression), Binary Cross entropy loss (sigmoid activation function w/ two mutually exclusive classes and single output neuron; aka log loss?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Cross Entropy loss (Note: did some extra reading outside the book)\n",
    "- used for multiple mutually exclusive classes in classification task, thus commonly used with a softmax activation layer\n",
    "- cross entropy means the differnce between two distributions, in our case, the output distribution of the network and the actual ground truth distribution.\n",
    "- categorical comes from the fact that the ground truth distribution is one-hot-encoded (i.e., 1 where true and 0 where false)\n",
    "- categorical cross entropy loss = -sum((ground_truth_value(i) * -log(predicted_value(i))); where i is the ith value in the softmax output matrix and the ground truth matrix is one hot encoded\n",
    "- one hot encoded is an array/matrix where the correct value or desired values are 1 and the rest are 0\n",
    "- so the above equation in categorical cross entropy loss results in all the wrong classes being multiplied by 0 and the correct class being multiplied by 1. This results in simplification in code to just -log(predicted_value_of_correct_class)\n",
    "\n",
    "Further math intuition => \n",
    "- -log(x) is downward sloping and where x = 1, -log(x) = 0; which works because if your network is predicting 1 for the correct class, then the loss is 0. => 1 * -log(0) = 0;\n",
    "- As error increases, the log loss approaches infinity, there is an asymptote at x = 0?; this will present a problem later in the book (need to add very small value to predicted probability so not passing 0) bc log(0) = undefined)\n",
    "\n",
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# Ground truth\n",
    "\n",
    "target_output = [1, 0, 0]\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "math.log(softmax_output[1])*target_output[1] +\n",
    "math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)\n",
    "\n",
    "#simplification - see notes above for explanation => don't need to include other terms besides one in desired \n",
    "# ground truth because they go to 0\n",
    "\n",
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
