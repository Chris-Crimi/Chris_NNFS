{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Network Error with Loss\n",
    "- loss function = cost function\n",
    "- loss calculates how wrong a network is from the correct answer and is the model's error. Thus, ideally loss should be zero.\n",
    "- Classification network outputs are akin to the confidence of the network's classification, and thus want to increase confidence (i.e., move correct neuron closer to 1) and decrease misplaced confidence\n",
    "- For the current task at hand, we will use categorical cross entropy loss, however for differeny types of network outputs, there are obviously different functions => Mean squared error (regression), Binary Cross entropy loss (sigmoid activation function w/ two mutually exclusive classes and single output neuron; aka log loss?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Cross Entropy loss (Note: did some extra reading outside the book)\n",
    "- used for multiple mutually exclusive classes in classification task, thus commonly used with a softmax activation layer\n",
    "- cross entropy means the differnce between two distributions, in our case, the output distribution of the network and the actual ground truth distribution.\n",
    "- categorical comes from the fact that the ground truth distribution is category based (i.e., there is only one correct category and not varying degrees of correctness/probability e.g., one hot encoded or sparse)\n",
    "- categorical cross entropy loss = -sum((ground_truth_value(i) * -log(predicted_value(i))); where i is the ith value in the softmax output matrix and the ground truth matrix is one hot encoded\n",
    "- one hot encoded is an array/matrix where the correct value or desired values are 1 and the rest are 0\n",
    "- so the above equation in categorical cross entropy loss results in all the wrong classes being multiplied by 0 and the correct class being multiplied by 1. This results in simplification in code to just -log(predicted_value_of_correct_class)\n",
    "- going forward references to log in the book mean natural log (ln = log with base e)\n",
    "- Ultimately => goal is to calculate average categorical cross entropy loss for each training batch\n",
    "\n",
    "Further math intuition => \n",
    "- -log(x) is downward sloping and where x = 1, -log(x) = 0; which works because if your network is predicting 1 for the correct class, then the loss is 0. => 1 * -log(0) = 0;\n",
    "- As confidence decreases (lower output value), the loss approaches infinity, there is an asymptote at x = 0; this will present a problem later in the book (need to add very small value to predicted probability so not passing 0) bc log(0) = undefined)\n",
    "\n",
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# Ground truth\n",
    "\n",
    "target_output = [1, 0, 0]\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "math.log(softmax_output[1])*target_output[1] +\n",
    "math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)\n",
    "\n",
    "#simplification - see notes above for explanation => don't need to include other terms besides one in desired \n",
    "# ground truth because they go to 0\n",
    "\n",
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamically Taking the Log of Desired Index Point \n",
    "- have layer output and the correct answer for the layer in an array and list\n",
    "- this list can be one-hot-encoded (explained and exemplified above), or sparse (below)\n",
    "- sparse means that ground-truth array contains numbers representing the correct classes, such as 0 = dog, 1 = cat, 2 = human. So [1, 0 , 2], would correspond with 3 feature samples, whose ground trought outputs are cat, dog, human. As opposed to one-hot-encoded where cat would be [0, 1, 0]. So a sparse array will be single dimension, whereas one hot encoded will be multi dimensonal\n",
    "- in the below examples, the loss is averaged, this also applies to one-hot, just did not show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n",
      "[0.7 0.5 0.9]\n",
      "[0.7 0.5 0.9]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "#where the value represents a class; e.g., 0 = dog, 1 = cat, 2 = human; so dog, cat, cat here\n",
    "class_targets = [0, 1, 1] #sparse encoding\n",
    "\n",
    "#one way to get this\n",
    "\n",
    "#for each row in the outputs, get the value in that row corresponding with the correct class, aka for each row, get column\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])\n",
    "\n",
    "#even faster using numpy => get the [[row_numbers, col_number]], getting each row here because we want each output\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])\n",
    "\n",
    "#so since we want to get the target value at each row, we always want to get each row, so can make further dynamic\n",
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "#range len counts off each row for the length of the softmax outputs array\n",
    "\n",
    "##full sparse simplification and log and average of loss => averaging applies to one-hot too\n",
    "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling One hot and sparse ground truth encodings\n",
    "- to make network as flexible as possible and be able to handle multiple ground truth formats (one-hot and sparse), are implementing the code below\n",
    "- can test whether the ground truth array is one hot or sparse by looking at dimensions; 2D array is one hot because each output row is a list of 1s and 0s for the hot and cold classes of the respective feature sample. Sparse is 1D because each value in array communicates the ground truth class for its respective feature sample, which is also designated to same the column (neuron) location index in the output array (see examples above), so if class is 0 then output position is the first spot in the row, so on an so forth \n",
    "- implementation: np arrays have property variable shape, which describes their dimensionality. If shape is tuple length of 1, then shape is 1D, if tuple length of 2 then shape is 2D, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "\n",
    "#implementation see notes above\n",
    "\n",
    "if len(class_targets.shape) == 1: #if 1D\n",
    "    correct_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "elif len(class_targets.shape) == 2: #if 2D\n",
    "    correct_confidences = np.sum(softmax_outputs*class_targets, axis = 1) #axis = 1 to sum the values of each row\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "\n",
    "print(average_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log of 0:\n",
    "- natural log function approaches 0, but never touches => intuitively, negative exponents are fractions, so the more negative an exponent gets, the closer the log gets to 0, but never touches\n",
    "- sometimes the network will return 0, if it has no confidence in the correct class. In this case, np.log returns -np.inf, which when put into averages (e.g, our average of lossses), will cause the average to also be infinity\n",
    "- just simply adding very small value to the condifence (e.g., ln(x + small_val)) to prevent 0 does not work because it shifts the function to the left, so when confidence is 100, the loss will be negative, which indicates negative error, and does not make sense. This overall biases confidence towards 1 as well, which could result in lower accuracy?\n",
    "- solution is to contrain the possible range of confidences to values between a very_small_value, and (1 - very_small_value), using np.clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000000e-07 9.999999e-01]\n"
     ]
    }
   ],
   "source": [
    "#clipping example\n",
    "y_pred = [0,1]\n",
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "print(y_pred_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for Loss\n",
    "- creating a common loss class and a child loss class for categorical cross entropy, presumably because we will have different loss functions later in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    def calculate(self, output, y): #output of softmax layer, y is y_true\n",
    "\n",
    "        sample_losses = self.forward(output, y) #forward function of child loss class\n",
    "        data_loss = np.mean(sample_losses) #average loss over batch\n",
    "        return data_loss\n",
    "    \n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss): #inherits loss class\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        samples = len(y_pred) #doing len in advance for 1D, not sure why its here and not in if statement but following book on this\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1: #if 1D aka sparse\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        elif len(y_true.shape) == 2: #if 2D aka 1 hot encoded\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis = 1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        \n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Code up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss 1.0986104\n",
      "acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # Return loss\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "    # Probabilities for target values -\n",
    "    # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "    \n",
    "    # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "    # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function (softmax)\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "# it takes the output of second dense layer activation and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(\"loss\", loss)\n",
    "\n",
    "###See below cells for further discussion on accuracy calculation\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Calculation:\n",
    "- accuracy measures the percentage of correctly identified classes; accuracy = number_correct/number_of_classes\n",
    "- to get network prediction for a sample, get the location of max of the softmax output array => so need to take the argmax value of each row in the batch. Can be done with np.argmax, which lets you specify a max on a specific axis, and returns the location of that max\n",
    "- softmax output is slightly different than above for the purpose of example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Getting the location in the output row of the predicted class\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "\n",
    "# If targets (ground truths) are one-hot encoded - convert them to 1D (also done with argmax for each row)\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1) #returns location in row of 1 hot encoded value\n",
    "\n",
    "#Returns True (1) where the locations are the same for predicted and class targets, meaning network was correct, False (0) otherwise\n",
    "#averaging this is accuracy bc count of correct = sum of true (1)\n",
    "accuracy = np.mean(predictions == class_targets) \n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
