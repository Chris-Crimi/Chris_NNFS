{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation\n",
    "- beginning with an simple example where we just seek to minimize the output of a single neuron\n",
    "- goal is to figure out how much each input, weight, and bias impacts the neuron function (and eventually then network)\n",
    "- to do this need to use the chain rule and take the derivative with respect to each input, weight, bias (only 1 bias here)\n",
    "- only use derivative with respect to weights and biases to minimize loss, but need to know derivatives with respect to inputs as well because it is used to chain to another layer (more understanding in next bullet point)\n",
    "- we are chaining the layers together via the input derivative so like derivative of each layer with respect to its input, but then on the last layer or the layer we are interested in calculating the derivative for, take the derivative of that layer with respect to weight or bias. Cause the change in the weight or bias ultimately is the ultimate input to that next layer, so it will transform that layer's input. dfunction/dweight = dlayer2(layer1output)/dlayer1 * dlayer1/dx; so its really the same thing as the chain rule, and as part of the chain rule, need to know the derivative of the outer function wih respect to its input, which is the inner function, then can take the derivative of inner function with respect to whatever parameter you want, in this case weight or bias. So the input is acting as the chain (the chain rule!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Prop on 1 Neuron\n",
    "- Example Neuron function where x0, w0 are inputs and respective weight, b is bias: y = relu(w0 * x0 + w1 * x1 + w2 *x2 + b) = max(w0 * x0 + w1 * x1 + w2 *x2 + b, 0)\n",
    "- can be broken down even further into considering each weight* input is own function; the book does this. See function, were sum() is the sum of the weights* inputs and bias, and mul() is weights* inputs. So derivative of full neuron with respect to x0 would be deriv from next layer wrt input * dReLU()/dsum() * dsum()/dmul() * dmul()/dx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Layer times Relu: 1.0\n",
      "Next Layer, RelU, and sum: 1.0 1.0 1.0 1.0\n",
      "Full wrt inputs, weights: -3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "# Backward pass\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "''' Example of how this comes together\n",
    "dtwoneurons/dx0 = dnext_layer/dReLU * dReLu/dsum() * dsum()/dmul() * dmul/dx0\n",
    "'''\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"Next Layer times Relu:\", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule, deriv of plain sum is just 1 (think about it)\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(\"Next Layer, RelU, and sum for each sum and bias:\", drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "#short cut to derivative here is that the deriv wrt to weight is just input value and wrt to input is just the weight (flip-flop)\n",
    "dmul_dx0 = w[0] \n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "#note that the full deriv wrt to bias is calculated above it is 1, since it is just a sum onto the sum function\n",
    "print(\"Full wrt inputs, weights:\",drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately Simplifying the Code Above\n",
    "- taking out all multplying by 1 etc. just leaves you with derivative of next_layer * ReLu * w0 (or x0, bias, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Layer times Relu: 1.0\n",
      "Full wrt inputs, weights: -3.0 1.0 -1.0 -2.0 2.0 3.0\n",
      "wrt bias 1.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "# Backward pass\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "''' Example of how this comes together\n",
    "dtwoneurons/dx0 = dnext_layer/dReLU * dReLu/dsum() * dsum()/dmul() * dmul/dx0\n",
    "\n",
    "z = sum() + b\n",
    "\n",
    "now simplified to dnext_layer/dReLU * dReLu/dz * dz/dx[0]\n",
    "'''\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"Next Layer times Relu:\", drelu_dz)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "#short cut to derivative here is that the deriv wrt to weight is just input value and wrt to input is just the weight (flip-flop)\n",
    "\n",
    "drelu_dx0 = dvalue * (1. if z > 0 else 0.) * w[0]\n",
    "drelu_dw0 = dvalue * (1. if z > 0 else 0.) * x[0]\n",
    "drelu_dx1 = dvalue * (1. if z > 0 else 0.) * w[1]\n",
    "drelu_dw1 = dvalue * (1. if z > 0 else 0.) * x[1]\n",
    "drelu_dx2 = dvalue * (1. if z > 0 else 0.) * w[2]\n",
    "drelu_dw2 = dvalue * (1. if z > 0 else 0.) * x[2]\n",
    "drelu_db = dvalue * (1. if z > 0 else 0.) * 1\n",
    "print(\"Full wrt inputs, weights:\",drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
    "print(\"wrt bias\", drelu_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing the output of neuron (manually); run cell above for gradient values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Weights [-3.0, -1.0, 2.0] 1.0\n",
      "New weights [-3.001, -0.998, 1.997] 0.999\n",
      "New Output 5.985 Old Output 6\n"
     ]
    }
   ],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2] # gradients on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2] # gradients on weights\n",
    "db = drelu_db # gradient on bias...just 1 bias here\n",
    "\n",
    "#current weights and bias\n",
    "print(\"Current Weights\", w, b)\n",
    "\n",
    "#applying a small negative value to our gradient to respect to weight, ie. how much the final output changes wrt to change in weights\n",
    "#negative because we want to decrease the output of neuron\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "#new weight and bias\n",
    "print(\"New weights\", w, b)\n",
    "\n",
    "# Multiplying inputs by new weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(\"New Output\", y, \"Old Output\", 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically gradient descent is this, which much of this insight came from reading outside the book:\n",
    "- find how much each weight impacts the loss/target function (derivative wrt weight aka gradient)\n",
    "- multiply that impact by a small constant amount across impacts (gradients) => so weights that have larger gradients will result in larger values, smaller gradients will result in smaller values (e.g., .001 * 1 < .001 * 2)\n",
    "- subtract that amount from the current weight value. Subtract because we are trying to minimize. Weights with larger impacts on function output (e.g., larger gradient) will decrease by more whereas ones with smaller gradients will decrease less\n",
    "- In other words: you are changing the weights that have the most impact by more than less impactful ones, thus \"moving in the steepest direction\" (as all the blogs say) because are changing the most impactful parameters by greater values. Adding/Subtracting a slope times some constant back to the slope so the steeper the slope (bigger gradient), the greater value you are adding/subtracting back to the slope(gradient), changing the slope by more than if it was less steep.\n",
    "- as the gradients decrease, then you make overall smaller changes to weights, so effectively taking smaller steps, the closer you get to optimizing the function at the minimum (gradients approaching zero)\n",
    "- changing the weights while holding input constant => by doing the above to change the most impactful weights by more to more quickly reduce the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the basic process of minimizing the output of a neuron, in reality want to minimize the loss of a network. The next layer aspect of this is kind of ingnored (it is 1), although if you include the next layer wouldn't we really be minimizing the value of the next layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Between Layers\n",
    "- so in a layer, each nueron outputs a gradient with respect to each input\n",
    "- the sum of the gradients for each respective input is the full derivative for that input. This leverages the principle that derivatives sum linearly. So partial derivative for input 1 across 3 neurons can sum togther for full derivative of input 1 for the whole layer => gained this insight thru additional reading outside the book\n",
    "- So here can just sum weights of each neuron that correspond to each respective input (excluding any other chain rule stuff from previous layer, otherwise need to multiply these first per chain rule) => this is the the shortcut approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Between Layers\n",
    "- Note: did not find the book explanation particularly clear, so the following is my interpretation of it\n",
    "- just like on the forward pass, each nueron is outputting a gradient with respect to inputs, on the backward pass, each following layer has a gradient with respect to the layer before it (i.e., previous layer occurs earlier in forward pass). To continue backpropagation/chain rule, need to calculate the full derivative of each of the previous layer's neurons to backpropagate through them. This is done by summing the following layer's partial derivatives with respect to that inputting neuron. Can do this because you can add derivatives (i.e. derivative of x + x + x is the same as 3x), in other words, partial derivatives wrt to same variable sum linearly/are additive.\n",
    "- Example: for neuron A in the current layer, all neurons in the following layer have a partial derivative associated with neuron A in the current layer. To do the chain rule and backpropagate through neuron A in the current layer, need to know the full derivative (i.e., not just partial) with respect to neuron A. A derivative is additive and the sum of the partial derivative (as seen in earlier chapters). Thus to get the full derivative wrt to neuron A's output, need to sum partial derivatives wrt to A' output of each of the following layer neurons.\n",
    "- a good visual metaphor is like bundling a bunch of frayed wires (partial derivatives) together into 1 big cable (full derivative)\n",
    "- so, given a following, current, and input layer: to get the partial derivatives wrt to an input layer neuron, need to do the chain rule etc. So multiply the following layer's derivative by the derivative wrt weight of the input neuron's output (aka just the input). This would be the weight that a current layer's neuron has on an input layer's neuron. This multiplication gives you the partial derivative wrt to that input neuron for a given current neuron. Repeat this for each neuron in the current layer and sum. This then gives you the full derivative wrt to that input neuron, which you can use to continue backpropagating thru the network. Repeat for all neurons in the layers.\n",
    "- (ignore if you are confused) basically you are calculating gradient wrt to the inputs of the layer, so each full derivative wrt to 1 input is part of a larger gradient of each of the input neurons output values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of Example from Book\n",
    "- imagine a layer of 3 neurons that have 4 inputs each (so 4 weights each). There are layers that follow the 3 layers, but these are not shown\n",
    "- we want to get the gradient of the function with respect to each of the 4 inputs. So we need the partial derivative of the function wrt to each of the inputs, aka need to take the derivative of the function wrt to each input\n",
    "- the layer of the 3 recives a gradient from the rest of the greater function , where each partial derivative is the derivative wrt each neuron's in the layer of 3's output. \n",
    "- so each of these partial derivatives is passed to its respective neuron in the layer of 3 to continue the chain rule.\n",
    "- the partial derivative is multiplied by the respective neuron weights (if taking deriv wrt to input), which results in each neuron have a gradient for each input\n",
    "- sum each neuron's partial derivative wrt to the same input to get the full derivative wrt to that input (explained above). That full derivative is a partial derivative to the larger gradient funtion of that layer\n",
    "- presumably this is also how we got the input gradient to the layer of 3 as well\n",
    "\n",
    "\n",
    "- nuance of the example is that the weight matrix is transposed because we have it naturally transposed (ie. it is defined transposed rather than being .T) in the actual layer objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for the loss funciton\n",
    "- the derivative is -( y_actual/ y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
