{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions\n",
    "- applied to the output of a nueron to modify the output\n",
    "- non-linear activation functions enable networks with two or more hidden layers to map to non-linear functions\n",
    "- typically two activations functions are used in the network => one type for all the neurons, and one type for the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Step Activation Function\n",
    "- y = 1 when x > 0; y = 0 when x <=0\n",
    "- a heavisde function\n",
    "- not typically used, but can be used for hidden layers\n",
    "- mimics firing/not firing\n",
    "- not an informative function as during training, adjustments to weights do not create change in output unless near the threshold; hard to tell how close it is to activation\n",
    "- the following functions are more informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear Activation Function\n",
    "- y=x\n",
    "- straight line\n",
    "- used for output layer (last layer's output) when network is used for regression to return scalar rather than classification\n",
    "- covered in more detail in chapter 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sigmoid Activation Function\n",
    "- y= 1 / (1 + e^(-x)) => function approaches 0 as x approaches negative infinity; is equal to .5 at x = 0, and approaches 1 as x approaches positive infinity\n",
    "- adds non-linearity\n",
    "- informative because each x value has a different y, unlike step function where 3 and 300k have the same value (1)\n",
    "- is less commonly used now, but was favored historically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rectified Linear Activation Function (ReLU)\n",
    "- y=x when x >0; y = 0 when x <= 0; y = max(x, 0)\n",
    "- non-linear because of the bend at 0\n",
    "- easier to compute than the sigmoid, so tends to be faster and more efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Use Activation Functions? / Linear Activation in the Hidden Layers\n",
    "- for NNs to fit non-linear data, requires non linear functions (aka non linear activation functions) and typically at least 2 hidden layers\n",
    "- NNs main attraction is solving non linear problems\n",
    "- using only linear activation functions results in only linear outputs => makes sense because if you only have linear neurons, then you are only performing linear transformations as you go through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU Activation in a Pair of Neurons / Further on features of ReLU\n",
    "- y = max(weights*inputs + biases, 0)\n",
    "- increasing the bias, with a positive weight, causes the neuron to activate in negative terriotry => intuitively, this is because negative inputs*weights can now be made more positive by greater bias\n",
    "- negating a positive weight mirrors the function over the y-axis. Makes sense as now more positive/less negative inputs will decrease the inputs*weights more, causing negative values for positive inputs. Creates a way to \"deactive\" a neuron, rather than activate. Because for inputs less than the bias, assuming weight is 1, then the neuron will always activate. Greater inputs will cause deactivation\n",
    "\n",
    "- found the way that the book walked through the two neurons confusing, I like it better if its done mathematically step by step => they add the second bias first, when I'd rather multiple the second weight first as that how it flows through network. My explanation to go along with the steps explanation is below (fig 4.12 to 4.18):\n",
    "1) First neuron has weight of -1 and bias of .5, so it deactivates at any values greater than .5\n",
    "2) Second neuron weight is -2, so slope of first neuron is now flipped to positive 2, and intersects y axis at -1 because first neuron intesected at .5. The potion of the neuron wher x > .5 remains zero because 0 * -2 = 0.\n",
    "3) Now add second neuron bias, the whole equation now shifts up 1, so it intersects y axis at 0, and the zero portion before is now 1. All values less than 0 are now 0 because of the second neuron Relu\n",
    "4) this results in a bull-spread style pay-off (finance/options reference) where between x = 0 and x =.5 , the combinations of neuron has a positive 2 slope. Greater than .5 and it is flat at 1, less than 0 and it is flat at 0 => the area where both neurons are active is an \"area of effect\" where changes to weights and biases impact the shape of the total function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU Activation in the Hidden Layers\n",
    "- in book: network with 2 hidden layers where neuron in 1st hidden layer is connected only to the following neuron in next layer (ie only one connection), so not fully connected\n",
    "- basically the example above\n",
    "- walking thru each step\n",
    "1) Top set (1st hidden pair) of neurons => just follow weights and biases thru using intuition of linear transformation from above. See book for notes: but whole network function is what they show. See margins for more intuitive diagrams adding up areas of effect\n",
    "2) Bottom set (8th hidden pair) of neurons biases the whole network up by .7, because bias of 1 in 2nd hidden layer times output weight of .7. This is a constant and does not have an impact on slopes or anything, so increases whole function output by .7, regardless of areas of effect, etc.\n",
    "3) 2nd hidden pair => Adding in the second pair of neurons increases the overall slope of final output function , such as by having slope (weight) of 1 this increases the slope of existing neurons, so 6x + 1x =7x. I think the book alludes to this by stating \"2nd pair of neuronsâ€™ activation is beginning too soon, which is impacting the 'area of effect' of the top pair that we already aligned.\" Thus, to make this function only active where we want it, need to add slight negative bias and increase slope. See book for further diagraming\n",
    "\n",
    "Generalization = > Adding in neuron pair changes the function. Use max() to create areas of effect for each neuron. Use biases to shift the area of effect around (e.g., for the sine approximation in the book, the neurons representing later in the since funciton have higher biases). Having multiple neuron pairs active at once is like adding two fucntions together.\n",
    "\n",
    "- definiting fucntion, adding bias, and taking max repeatedly and alternating between negative and positive slopes creates areas where the neuron output is 0, which creates areas of effect\n",
    "\n",
    "- making a fully connected network is difficult to follow, but has the same principles \n",
    "- full connections with more neurons increases the ability of the network to finely fit the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu Activation Function Code\n",
    "- Relu is simply a max function, where it is the greater of the output of the neuron and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#numpy maximum example\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "outputs = np.maximum(0, inputs)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adding Relu into existing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        #calculate output values\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "###### NEW RELU ACTIVATION FUNCTION CLASS #########\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "######################################\n",
    "\n",
    "\n",
    "#create dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "#create layer that accepts 2 input features and has 3 neurons\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "#Create ReLU activation layer\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#forward pass through dense1\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "\n",
    "print(activation1.output[:5]) #results in values of 0 or greater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Softmax Activation Function\n",
    "- softmax_output = e^z(i)/(sum(j to K; e^z(j))); e is eulers number\n",
    "- for each value in the output array, exponentiate it, and divide it by the sum of all the exponentiated values in the output array\n",
    "can be annotated as sigma(z), or S\n",
    "- functionally this is used for classification. By dividing output values by sum of output values, this normalizes all the outputs between 0 and 1, giving the network's probability estimates of the correct class. Further, exponentiating transforms negative values into positive values (negative inputs to e are very low positive numbers).\n",
    "\n",
    "Did some extra curricular research => if you add a coefficient infront of z (but still in exponent), called Beta, this controls the \"uniformity\" of the output distribution. Lower Beta values increase uniformity, so lower/more negative output values recieve higher weightings than under Beta = 1. Temperature is the inverse of Beta (1/ Beta), so smaller Beta = higher temp. I wonder if this is how LLMs control their temperature setting => increasing beta of output layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
