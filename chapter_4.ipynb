{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions\n",
    "- applied to the output of a nueron to modify the output\n",
    "- non-linear activation functions enable networks with two or more hidden layers to map to non-linear functions\n",
    "- typically two activations functions are used in the network => one type for all the neurons, and one type for the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Step Activation Function\n",
    "- y = 1 when x > 0; y = 0 when x <=0\n",
    "- a heavisde function\n",
    "- not typically used, but can be used for hidden layers\n",
    "- mimics firing/not firing\n",
    "- not an informative function as during training, adjustments to weights do not create change in output unless near the threshold; hard to tell how close it is to activation\n",
    "- the following functions are more informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear Activation Function\n",
    "- y=x\n",
    "- straight line\n",
    "- used for output layer (last layer's output) when network is used for regression to return scalar rather than classification\n",
    "- covered in more detail in chapter 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sigmoid Activation Function\n",
    "- y= 1 / (1 + e^(-x)) => function approaches 0 as x approaches negative infinity; is equal to .5 at x = 0, and approaches 1 as x approaches positive infinity\n",
    "- adds non-linearity\n",
    "- informative because each x value has a different y, unlike step function where 3 and 300k have the same value (1)\n",
    "- is less commonly used now, but was favored historically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rectified Linear Activation Function (ReLU)\n",
    "- y=x when x >0; y = 0 when x <= 0; y = max(x, 0)\n",
    "- non-linear because of the bend at 0\n",
    "- easier to compute than the sigmoid, so tends to be faster and more efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Use Activation Functions? / Linear Activation in the Hidden Layers\n",
    "- for NNs to fit non-linear data, requires non linear functions (aka non linear activation functions) and typically at least 2 hidden layers\n",
    "- NNs main attraction is solving non linear problems\n",
    "- using only linear activation functions results in only linear outputs => makes sense because if you only have linear neurons, then you are only performing linear transformations as you go through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU Activation in a Pair of Neurons / Further on features of ReLU\n",
    "- y = max(weights*inputs + biases, 0)\n",
    "- increasing the bias, with a positive weight, causes the neuron to activate in negative terriotry => intuitively, this is because negative inputs*weights can now be made more positive by greater bias\n",
    "- negating a positive weight mirrors the function over the y-axis. Makes sense as now more positive/less negative inputs will decrease the inputs*weights more, causing negative values for positive inputs. Creates a way to \"deactive\" a neuron, rather than activate. Because for inputs less than the bias, assuming weight is 1, then the neuron will always activate. Greater inputs will cause deactivation\n",
    "\n",
    "- found the way that the book walked through the two neurons confusing, I like it better if its done mathematically step by step => they add the second bias first, when I'd rather multiple the second weight first as that how it flows through network. My explanation to go along with the steps explanation is below (fig 4.12 to 4.18):\n",
    "1) First neuron has weight of -1 and bias of .5, so it deactivates at any values greater than .5\n",
    "2) Second neuron weight is -2, so slope of first neuron is now flipped to positive 2, and intersects y axis at -1 because first neuron intesected at .5. The potion of the neuron wher x > .5 remains zero because 0 * -2 = 0.\n",
    "3) Now add second neuron bias, the whole equation now shifts up 1, so it intersects y axis at 0, and the zero portion before is now 1. All values less than 0 are now 0 because of the second neuron Relu\n",
    "4) this results in a bull-spread style pay-off (finance/options reference) where between x = 0 and x =.5 , the combinations of neuron has a positive 2 slope. Greater than .5 and it is flat at 1, less than 0 and it is flat at 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
