{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Logistic Regression\n",
    "- A situation where the variable you are trying to predict is binary (1/0). So it can be either 0 or 1. for example, if you have a neural network that tries to predict whether there is cat in a picture, there will be an output neuron that is very close to 1 when cat, and close to 0 when not cat.\n",
    "- in a neural network this is represented by a single neuron. 1 for present feature, 0 for target feature not present. But can have multiple of these neurons in the output layer. For example, one neuron could be person, not person and the other could be indoors/outdoors.\n",
    "- uses sigmoid activation function rather than softmax, and binary cross entropy rather than categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Activation Function\n",
    "- sigma(x) = 1 / (1 + e^-x)\n",
    "- squishes the range of possible input values between 0 and 1, so very negative values are very close to 0, and very positive values are close to 1\n",
    "- Derivative of sigmoid function => will be written in terms of the sigmoid function, similar to how we did softmax derivative\n",
    "1) re-write = d/dz (1/(1 + e^-z)) = (1 + e^-z)^-1\n",
    "2) use chain rule: -1 * ( 1+ e^-z) ^ -2 * d/dz (1 + e^-z) = -1 * ( 1+ e^-z) ^ -2 * (0 + e^-z * -1)\n",
    "3) simplify, cancel negatives = ( 1+ e^-z) ^ -2 * e^-z\n",
    "4) algebra: 1/ (1 + e^-z)^2 * e^-z = 1/ (1 + e^-z) * e^-z / (1 + e^-z)  = 1/ (1 + e^-z) * (1+ e^-z - 1) / (1 + e^-z) = 1/ (1 + e^-z) * ((1+ e^-z)/ (1 + e^-z) - 1/(1 + e^-z)) = 1/ (1 + e^-z) * (1 - 1/(1 + e^-z)) = sigma(x) * (1 - sigma(x))\n",
    "- remeber sigmoid function is equal to 1 / (1 + e^-x), so we are just substituting it in above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid object  \n",
    "- we save output so we can just re use it in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy Loss\n",
    "- similar to categorical cross entropy loss\n",
    "- Before we get started, remember: a single neuron represents two classes. So we the loss of that neuron will represent the loss of two classes\n",
    "- Lij = yij * -ln(y_hatij) + (1 - yij) * -ln(1 - y_hatij) => yij is the ground truth and y_hat ij is the predicted value.\n",
    "- So in the equation above, since the predicted class is binary, we can define the opposing class as 1 - probablity of class. Whichever class is ground truth, will be multiplied by 1, and whichever one is incorrect will be changed to 0 via the yij/(1 - yij terms), then you will just be left with the loss of the other class\n",
    "- However, if we have multiple outputs, we will receive a vector of losses for a sample (unlike categorical cross entropy). For example, if we are checking the existenece of two classes, we will have a single loss value associated with each output node. So we will not have one loss value for the whole function. To solve this, we will take the simple average loss of the output values for a sample. in other words, we will take the mean of the losses for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross Entropy Loss Derivative:\n",
    "- Lij = yij * -ln(y_hatij) + (1 - yij) * -ln(1 - y_hatij) => rearrange to:\n",
    "- = -yij * ln(y_hatij) - (1 - yij) + ln(1 - y_hatij) => take derivative of loss wrt to y_hatij, split each term into its own sum deriv for ease\n",
    "= d/dy_hatij (-yij * ln(y_hatij)) + d/dy_hatij(-(1 - yij) + ln(1 - y_hatij)) => move constants outside\n",
    "= -yij * d/dy_hatij (ln(y_hatij)) - (1 - yij) *d/dy_hatij * ln(1 - y_hatij) = > derivative first step\n",
    "= -yij * 1/y_hatij * d/dy_hatij * y_hatij - (1 - yij) * 1/(1 - y_hatij) * d/dy_hatij (1 - y_hatij)\n",
    "= -yij * 1/y_hatij * 1 - (1 - yij) * 1/(1 - y_hatij) * (0 - 1) => rearrange\n",
    "= -(yij/y_hatij - (1 - yij)/(1 - y_hatij))\n",
    "\n",
    "Don't forget the average portion though\n",
    "- = (1/j) * Sum(Sample_losses), j = number of output neurons\n",
    "- Li = (1/j) * Sum(Sample_lossesj)\n",
    "- dLi/dy_hatij = dLi/dLij * dLij/dy_hatij => we've already solved dLij/dy_hatij above, so now just need dLi/dLij\n",
    "- dLi/dLij = (1/j) * Sum(Sample_lossesij) => all the non losses that we are not deriving wrt to, e.g. k /= j (similar to softmax loss if you remember), drop to 0, and the k = j, = 1, so the whole derivative simplifies to\n",
    "- dLi/dLij = 1/j\n",
    "\n",
    "Our full derivative:\n",
    "- = dLi/dy_hatij = dLi/dLij * dLij/dy_hatij\n",
    "- = 1/j * -(yij/y_hatij - (1 - yij)/(1 - y_hatij))\n",
    "- = -1/j * -(yij/y_hatij - (1 - yij)/(1 - y_hatij))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Implementation:\n",
    "- note when we average the gradients across the sample, we use np.mean(sample_losses, axis = -1), the axis = -1, is to average across the last axis, aka the rows, which represent each sample.\n",
    "- our binary cross entropy loss object has similar construction to categorical cross entropy, in that we need to clip our predicted values to prevent -ln(0), or some very high number, which throws -inf/+inf\n",
    "- then we also normalize the gradients as we did before by averaging them for the whole batch; remeber - it is to prevent higher gradients for larger batches\n",
    "- NOTE: if you want the cell below to run, you need to run the last cell in the workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        \n",
    "        #note the negative -1 axis\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples - for averaging the loss across the batch (e.g., normalzing)\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0]) # for the 1/J in the binary cross entropy loss deriv\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Out Binary Cross Entropy\n",
    "- Note: we have to reshape the training and testing data, because it is returned in just a simple list; however, we need a list of lists, where the bigger list is each sample's ground truth outputs, and the inside list is the ground truth values for each output neuron. in this example, we only have one neuron so the it is a list of single item lists\n",
    "- when calculating our training and validation accuracy, we use the following construction: predictions = (activation2.output > 0.5) * 1, then accuracy = np.mean(predictions == y). The > comparison returns boolean true/false values, and multiplying by 1 converts it to 1/0 values\n",
    "- model performs fairly well with settings shown below.\n",
    "- BE SURE TO RUN LAST CELL in WORKBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.500, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100, acc: 0.630, loss: 0.674 (data_loss: 0.673, reg_loss: 0.001), lr: 0.0009999505024501287\n",
      "epoch: 200, acc: 0.625, loss: 0.669 (data_loss: 0.668, reg_loss: 0.001), lr: 0.0009999005098992651\n",
      "epoch: 300, acc: 0.650, loss: 0.664 (data_loss: 0.663, reg_loss: 0.002), lr: 0.000999850522346909\n",
      "epoch: 400, acc: 0.650, loss: 0.659 (data_loss: 0.657, reg_loss: 0.002), lr: 0.0009998005397923115\n",
      "epoch: 500, acc: 0.675, loss: 0.647 (data_loss: 0.644, reg_loss: 0.004), lr: 0.0009997505622347225\n",
      "epoch: 600, acc: 0.720, loss: 0.632 (data_loss: 0.625, reg_loss: 0.006), lr: 0.0009997005896733929\n",
      "epoch: 700, acc: 0.770, loss: 0.614 (data_loss: 0.603, reg_loss: 0.010), lr: 0.0009996506221075735\n",
      "epoch: 800, acc: 0.775, loss: 0.593 (data_loss: 0.579, reg_loss: 0.015), lr: 0.000999600659536515\n",
      "epoch: 900, acc: 0.785, loss: 0.575 (data_loss: 0.555, reg_loss: 0.020), lr: 0.0009995507019594694\n",
      "epoch: 1000, acc: 0.785, loss: 0.559 (data_loss: 0.535, reg_loss: 0.024), lr: 0.000999500749375687\n",
      "epoch: 1100, acc: 0.790, loss: 0.546 (data_loss: 0.519, reg_loss: 0.028), lr: 0.0009994508017844195\n",
      "epoch: 1200, acc: 0.790, loss: 0.534 (data_loss: 0.503, reg_loss: 0.031), lr: 0.0009994008591849186\n",
      "epoch: 1300, acc: 0.790, loss: 0.524 (data_loss: 0.490, reg_loss: 0.034), lr: 0.0009993509215764362\n",
      "epoch: 1400, acc: 0.790, loss: 0.513 (data_loss: 0.476, reg_loss: 0.037), lr: 0.0009993009889582235\n",
      "epoch: 1500, acc: 0.805, loss: 0.503 (data_loss: 0.464, reg_loss: 0.039), lr: 0.0009992510613295335\n",
      "epoch: 1600, acc: 0.805, loss: 0.494 (data_loss: 0.453, reg_loss: 0.041), lr: 0.0009992011386896176\n",
      "epoch: 1700, acc: 0.820, loss: 0.483 (data_loss: 0.440, reg_loss: 0.043), lr: 0.0009991512210377285\n",
      "epoch: 1800, acc: 0.820, loss: 0.475 (data_loss: 0.431, reg_loss: 0.044), lr: 0.0009991013083731183\n",
      "epoch: 1900, acc: 0.825, loss: 0.468 (data_loss: 0.422, reg_loss: 0.046), lr: 0.0009990514006950402\n",
      "epoch: 2000, acc: 0.825, loss: 0.462 (data_loss: 0.415, reg_loss: 0.047), lr: 0.0009990014980027463\n",
      "epoch: 2100, acc: 0.835, loss: 0.455 (data_loss: 0.407, reg_loss: 0.048), lr: 0.0009989516002954898\n",
      "epoch: 2200, acc: 0.835, loss: 0.449 (data_loss: 0.401, reg_loss: 0.049), lr: 0.000998901707572524\n",
      "epoch: 2300, acc: 0.835, loss: 0.444 (data_loss: 0.394, reg_loss: 0.050), lr: 0.0009988518198331018\n",
      "epoch: 2400, acc: 0.840, loss: 0.438 (data_loss: 0.387, reg_loss: 0.051), lr: 0.0009988019370764769\n",
      "epoch: 2500, acc: 0.855, loss: 0.430 (data_loss: 0.379, reg_loss: 0.052), lr: 0.0009987520593019025\n",
      "epoch: 2600, acc: 0.860, loss: 0.425 (data_loss: 0.373, reg_loss: 0.053), lr: 0.000998702186508632\n",
      "epoch: 2700, acc: 0.860, loss: 0.421 (data_loss: 0.368, reg_loss: 0.053), lr: 0.00099865231869592\n",
      "epoch: 2800, acc: 0.860, loss: 0.417 (data_loss: 0.363, reg_loss: 0.054), lr: 0.0009986024558630198\n",
      "epoch: 2900, acc: 0.860, loss: 0.413 (data_loss: 0.358, reg_loss: 0.054), lr: 0.0009985525980091856\n",
      "epoch: 3000, acc: 0.865, loss: 0.409 (data_loss: 0.354, reg_loss: 0.055), lr: 0.000998502745133672\n",
      "epoch: 3100, acc: 0.870, loss: 0.405 (data_loss: 0.350, reg_loss: 0.055), lr: 0.0009984528972357331\n",
      "epoch: 3200, acc: 0.870, loss: 0.401 (data_loss: 0.345, reg_loss: 0.056), lr: 0.0009984030543146237\n",
      "epoch: 3300, acc: 0.870, loss: 0.396 (data_loss: 0.339, reg_loss: 0.056), lr: 0.0009983532163695982\n",
      "epoch: 3400, acc: 0.870, loss: 0.392 (data_loss: 0.335, reg_loss: 0.057), lr: 0.000998303383399912\n",
      "epoch: 3500, acc: 0.870, loss: 0.389 (data_loss: 0.331, reg_loss: 0.058), lr: 0.0009982535554048193\n",
      "epoch: 3600, acc: 0.875, loss: 0.383 (data_loss: 0.326, reg_loss: 0.058), lr: 0.000998203732383576\n",
      "epoch: 3700, acc: 0.885, loss: 0.379 (data_loss: 0.321, reg_loss: 0.058), lr: 0.0009981539143354365\n",
      "epoch: 3800, acc: 0.890, loss: 0.376 (data_loss: 0.317, reg_loss: 0.059), lr: 0.0009981041012596574\n",
      "epoch: 3900, acc: 0.890, loss: 0.373 (data_loss: 0.313, reg_loss: 0.059), lr: 0.0009980542931554933\n",
      "epoch: 4000, acc: 0.890, loss: 0.370 (data_loss: 0.310, reg_loss: 0.060), lr: 0.0009980044900222008\n",
      "epoch: 4100, acc: 0.890, loss: 0.367 (data_loss: 0.306, reg_loss: 0.060), lr: 0.0009979546918590348\n",
      "epoch: 4200, acc: 0.890, loss: 0.363 (data_loss: 0.302, reg_loss: 0.060), lr: 0.0009979048986652524\n",
      "epoch: 4300, acc: 0.890, loss: 0.359 (data_loss: 0.298, reg_loss: 0.061), lr: 0.000997855110440109\n",
      "epoch: 4400, acc: 0.895, loss: 0.355 (data_loss: 0.293, reg_loss: 0.061), lr: 0.0009978053271828614\n",
      "epoch: 4500, acc: 0.910, loss: 0.346 (data_loss: 0.285, reg_loss: 0.061), lr: 0.0009977555488927658\n",
      "epoch: 4600, acc: 0.905, loss: 0.340 (data_loss: 0.278, reg_loss: 0.062), lr: 0.000997705775569079\n",
      "epoch: 4700, acc: 0.910, loss: 0.330 (data_loss: 0.268, reg_loss: 0.062), lr: 0.0009976560072110577\n",
      "epoch: 4800, acc: 0.920, loss: 0.326 (data_loss: 0.263, reg_loss: 0.063), lr: 0.0009976062438179587\n",
      "epoch: 4900, acc: 0.920, loss: 0.322 (data_loss: 0.259, reg_loss: 0.064), lr: 0.0009975564853890394\n",
      "epoch: 5000, acc: 0.920, loss: 0.319 (data_loss: 0.255, reg_loss: 0.065), lr: 0.000997506731923557\n",
      "epoch: 5100, acc: 0.920, loss: 0.316 (data_loss: 0.251, reg_loss: 0.065), lr: 0.0009974569834207687\n",
      "epoch: 5200, acc: 0.930, loss: 0.313 (data_loss: 0.247, reg_loss: 0.066), lr: 0.0009974072398799322\n",
      "epoch: 5300, acc: 0.935, loss: 0.310 (data_loss: 0.244, reg_loss: 0.066), lr: 0.0009973575013003048\n",
      "epoch: 5400, acc: 0.935, loss: 0.308 (data_loss: 0.241, reg_loss: 0.067), lr: 0.0009973077676811448\n",
      "epoch: 5500, acc: 0.935, loss: 0.305 (data_loss: 0.238, reg_loss: 0.067), lr: 0.00099725803902171\n",
      "epoch: 5600, acc: 0.935, loss: 0.303 (data_loss: 0.235, reg_loss: 0.067), lr: 0.0009972083153212581\n",
      "epoch: 5700, acc: 0.935, loss: 0.300 (data_loss: 0.233, reg_loss: 0.068), lr: 0.000997158596579048\n",
      "epoch: 5800, acc: 0.935, loss: 0.298 (data_loss: 0.230, reg_loss: 0.068), lr: 0.0009971088827943377\n",
      "epoch: 5900, acc: 0.935, loss: 0.296 (data_loss: 0.227, reg_loss: 0.068), lr: 0.0009970591739663862\n",
      "epoch: 6000, acc: 0.935, loss: 0.293 (data_loss: 0.225, reg_loss: 0.068), lr: 0.0009970094700944517\n",
      "epoch: 6100, acc: 0.940, loss: 0.291 (data_loss: 0.223, reg_loss: 0.069), lr: 0.0009969597711777935\n",
      "epoch: 6200, acc: 0.940, loss: 0.289 (data_loss: 0.220, reg_loss: 0.069), lr: 0.00099691007721567\n",
      "epoch: 6300, acc: 0.940, loss: 0.287 (data_loss: 0.218, reg_loss: 0.068), lr: 0.000996860388207341\n",
      "epoch: 6400, acc: 0.945, loss: 0.284 (data_loss: 0.216, reg_loss: 0.068), lr: 0.0009968107041520655\n",
      "epoch: 6500, acc: 0.945, loss: 0.282 (data_loss: 0.214, reg_loss: 0.068), lr: 0.000996761025049103\n",
      "epoch: 6600, acc: 0.950, loss: 0.279 (data_loss: 0.211, reg_loss: 0.068), lr: 0.000996711350897713\n",
      "epoch: 6700, acc: 0.955, loss: 0.272 (data_loss: 0.203, reg_loss: 0.069), lr: 0.0009966616816971556\n",
      "epoch: 6800, acc: 0.955, loss: 0.269 (data_loss: 0.200, reg_loss: 0.069), lr: 0.00099661201744669\n",
      "epoch: 6900, acc: 0.960, loss: 0.266 (data_loss: 0.197, reg_loss: 0.069), lr: 0.0009965623581455767\n",
      "epoch: 7000, acc: 0.960, loss: 0.264 (data_loss: 0.195, reg_loss: 0.069), lr: 0.000996512703793076\n",
      "epoch: 7100, acc: 0.960, loss: 0.262 (data_loss: 0.193, reg_loss: 0.069), lr: 0.0009964630543884481\n",
      "epoch: 7200, acc: 0.960, loss: 0.261 (data_loss: 0.191, reg_loss: 0.069), lr: 0.0009964134099309536\n",
      "epoch: 7300, acc: 0.960, loss: 0.259 (data_loss: 0.190, reg_loss: 0.069), lr: 0.0009963637704198528\n",
      "epoch: 7400, acc: 0.960, loss: 0.257 (data_loss: 0.188, reg_loss: 0.069), lr: 0.0009963141358544066\n",
      "epoch: 7500, acc: 0.965, loss: 0.256 (data_loss: 0.187, reg_loss: 0.069), lr: 0.000996264506233876\n",
      "epoch: 7600, acc: 0.965, loss: 0.254 (data_loss: 0.186, reg_loss: 0.068), lr: 0.0009962148815575223\n",
      "epoch: 7700, acc: 0.965, loss: 0.253 (data_loss: 0.185, reg_loss: 0.068), lr: 0.000996165261824606\n",
      "epoch: 7800, acc: 0.965, loss: 0.251 (data_loss: 0.183, reg_loss: 0.068), lr: 0.0009961156470343895\n",
      "epoch: 7900, acc: 0.965, loss: 0.250 (data_loss: 0.182, reg_loss: 0.067), lr: 0.0009960660371861334\n",
      "epoch: 8000, acc: 0.965, loss: 0.248 (data_loss: 0.181, reg_loss: 0.067), lr: 0.0009960164322790998\n",
      "epoch: 8100, acc: 0.965, loss: 0.247 (data_loss: 0.180, reg_loss: 0.067), lr: 0.0009959668323125503\n",
      "epoch: 8200, acc: 0.965, loss: 0.246 (data_loss: 0.179, reg_loss: 0.066), lr: 0.000995917237285747\n",
      "epoch: 8300, acc: 0.960, loss: 0.244 (data_loss: 0.179, reg_loss: 0.066), lr: 0.000995867647197952\n",
      "epoch: 8400, acc: 0.960, loss: 0.243 (data_loss: 0.178, reg_loss: 0.065), lr: 0.0009958180620484277\n",
      "epoch: 8500, acc: 0.960, loss: 0.242 (data_loss: 0.177, reg_loss: 0.065), lr: 0.0009957684818364362\n",
      "epoch: 8600, acc: 0.960, loss: 0.240 (data_loss: 0.176, reg_loss: 0.065), lr: 0.0009957189065612402\n",
      "epoch: 8700, acc: 0.960, loss: 0.236 (data_loss: 0.171, reg_loss: 0.065), lr: 0.000995669336222102\n",
      "epoch: 8800, acc: 0.960, loss: 0.234 (data_loss: 0.169, reg_loss: 0.065), lr: 0.000995619770818285\n",
      "epoch: 8900, acc: 0.965, loss: 0.233 (data_loss: 0.168, reg_loss: 0.065), lr: 0.0009955702103490519\n",
      "epoch: 9000, acc: 0.965, loss: 0.231 (data_loss: 0.166, reg_loss: 0.065), lr: 0.000995520654813666\n",
      "epoch: 9100, acc: 0.965, loss: 0.230 (data_loss: 0.165, reg_loss: 0.065), lr: 0.0009954711042113903\n",
      "epoch: 9200, acc: 0.965, loss: 0.229 (data_loss: 0.164, reg_loss: 0.064), lr: 0.0009954215585414883\n",
      "epoch: 9300, acc: 0.965, loss: 0.227 (data_loss: 0.163, reg_loss: 0.064), lr: 0.000995372017803224\n",
      "epoch: 9400, acc: 0.965, loss: 0.226 (data_loss: 0.162, reg_loss: 0.064), lr: 0.0009953224819958604\n",
      "epoch: 9500, acc: 0.965, loss: 0.225 (data_loss: 0.161, reg_loss: 0.064), lr: 0.000995272951118662\n",
      "epoch: 9600, acc: 0.965, loss: 0.224 (data_loss: 0.160, reg_loss: 0.064), lr: 0.0009952234251708924\n",
      "epoch: 9700, acc: 0.965, loss: 0.223 (data_loss: 0.159, reg_loss: 0.064), lr: 0.000995173904151816\n",
      "epoch: 9800, acc: 0.965, loss: 0.222 (data_loss: 0.158, reg_loss: 0.063), lr: 0.0009951243880606966\n",
      "epoch: 9900, acc: 0.965, loss: 0.221 (data_loss: 0.157, reg_loss: 0.063), lr: 0.0009950748768967994\n",
      "epoch: 10000, acc: 0.965, loss: 0.219 (data_loss: 0.156, reg_loss: 0.063), lr: 0.0009950253706593885\n",
      "validation, acc: 0.945, loss: 0.207\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Sigmoid activation:\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "    \n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    \n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Part in the brackets returns a binary mask - array consisting\n",
    "    # of True/False values, multiplying it by 1 changes it into array\n",
    "    # of 1s and 0s\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "        f'acc: {accuracy:.3f}, '+\n",
    "        f'loss: {loss:.3f} (' +\n",
    "        f'data_loss: {data_loss:.3f}, ' +\n",
    "        f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "        f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# Validate the model\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate the data loss\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Part in the brackets returns a binary mask - array consisting of\n",
    "# True/False values, multiplying it by 1 changes it into array\n",
    "# of 1s and 0s\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Network Object Code up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0, \n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from input ones, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "        # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "        size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "        # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # letâ€™s make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "# Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "    # 0 by default\n",
    "        regularization_loss = 0\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "        return regularization_loss\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "        \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "# Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "    # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # For each row in dinputs, get what the network has for the correct class and subtract 1\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "        (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        \n",
    "        #note the negative -1 axis\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples - for averaging the loss across the batch (e.g., normalzing)\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0]) # for the 1/J in the binary cross entropy loss deriv\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "    # If we use momentum\n",
    "        if self.momentum:\n",
    "        # If layer does not contain momentum arrays, create them\n",
    "        # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=.001, decay=0., epsilon=1e-7, rho=.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay*self.iterations))\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "    \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "    # If layer does not contain cache arrays,\n",
    "    # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
