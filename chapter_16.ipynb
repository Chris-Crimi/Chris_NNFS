{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Logistic Regression\n",
    "- A situation where the variable you are trying to predict is binary (1/0). So it can be either 0 or 1. for example, if you have a neural network that tries to predict whether there is cat in a picture, there will be an output neuron that is very close to 1 when cat, and close to 0 when not cat.\n",
    "- in a neural network this is represented by a single neuron. 1 for present feature, 0 for target feature not present. But can have multiple of these neurons in the output layer. For example, one neuron could be person, not person and the other could be indoors/outdoors.\n",
    "- uses sigmoid activation function rather than softmax, and binary cross entropy rather than categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Activation Function\n",
    "- sigma(x) = 1 / (1 + e^-x)\n",
    "- squishes the range of possible input values between 0 and 1, so very negative values are very close to 0, and very positive values are close to 1\n",
    "- Derivative of sigmoid function => will be written in terms of the sigmoid function, similar to how we did softmax derivative\n",
    "1) re-write = d/dz (1/(1 + e^-z)) = (1 + e^-z)^-1\n",
    "2) use chain rule: -1 * ( 1+ e^-z) ^ -2 * d/dz (1 + e^-z) = -1 * ( 1+ e^-z) ^ -2 * (0 + e^-z * -1)\n",
    "3) simplify, cancel negatives = ( 1+ e^-z) ^ -2 * e^-z\n",
    "4) algebra: 1/ (1 + e^-z)^2 * e^-z = 1/ (1 + e^-z) * e^-z / (1 + e^-z)  = 1/ (1 + e^-z) * (1+ e^-z - 1) / (1 + e^-z) = 1/ (1 + e^-z) * ((1+ e^-z)/ (1 + e^-z) - 1/(1 + e^-z)) = 1/ (1 + e^-z) * (1 - 1/(1 + e^-z)) = sigma(x) * (1 - sigma(x))\n",
    "- remeber sigmoid function is equal to 1 / (1 + e^-x), so we are just substituting it in above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid object  \n",
    "- we save output so we can just re use it in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
