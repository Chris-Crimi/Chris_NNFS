{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model Object - creating an object to manage all the network objects. This will help with reducing the repeatability of the code. I'm going to just walk through all the individual steps the book walks through instead of providing a summary of the whole model object.\n",
    "- Overall having a model object makes it easier to store and load the model for future prediction and training; it will also make it easier to build new models by cutting down on coding required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - the basic setup:\n",
    "- Create model object that is defined with an empty list attribute that holds the layers\n",
    "- each layer is added to the list via an add method \n",
    "- add a set method that sets various features of the model such as the loss function and the type of optimizer to use\n",
    "    - note: after the self parameter in the set method there is an \"*\" parameter. This makes it so that all of the following parameters are required to be defined by name explicilty. For example , you cannot just do set(BinaryCrossEntropy, Optimizer_SGD), it must be set(loss=BinaryCrossEntropy, optimizer = Optimizer_SGD). This to enforce better read-ability in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers() ##holds all the layers that we then input using the add method\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) #adds the layer to the self.layers list\n",
    "    def set(self, *, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - adding forward pass:\n",
    "- there are a couple of intermediate steps that we have to do before we can do a full forward pass. First we must create a input layer that exists soley to be an object to facilitate the use of a for loop in the forward pass. The construction of this object, just makes it so that our inputs can now be referenced as a self.outputs to make the for loop work. Because all the other layers have a self.output attribute.\n",
    "- then we must also create a finalize method that takes all the model layers we have added to the model object and sets them up for the forward and backward passes by assigning new facilitating attributes. The finalize method iterates over the layers list (which excludes the Layer_Input), and defines a new attribute for each layer class in our layers list. The attributes are either \"prev\" or \"next\", where prev is the previous layer and next is the next layer. For the first layer, the previous layer is the input layer; for the last layer, the next layer is the loss function. This is so that later in the code when we do forward pass, we can just iterate over the layers and call the layer.prev.output, which will get us the output of the previous layer. these prev and next references are references to the other layer objects to they don't create additional memory usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new input layer class\n",
    "\n",
    "class Layer_Input:\n",
    "    def forward(self, inputs):\n",
    "        self.output = inputs #making so that we can reference the inputs as .output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding the finalize method\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers() ##holds all the layers that we then input using the add method\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) #adds the layer to the self.layers list\n",
    "    def set(self, *, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    ###ADDING finalize method\n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input() #NOT INCLUDED IN THE layer_count below\n",
    "\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1] \n",
    "            \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: adding the forward method for forward pass\n",
    "- this requires two additional methods - the forward method, and the train method. The train method is what actually iterates over the training data for the user defined epochs and it uses the model object's defined forward method.\n",
    "- there is kind of a hierarchy of forward method calls here where the train method calls the model object's forward method which calls each layer object's forward method. The model's forward object uses layer.prev.output to pass the previous layer's outputs to the current layer, hence our need for the Layer Input class => our inputs need to have an .output attribute. The end of the model's forward method calls layer.output attribute for the network's final output. This is because the layer variable is defined in the for loop. So it's just taking the last layer from the for loop's output. Note that this does not include loss, which is defined later.\n",
    "- the train function is largely just for show right now, it will have more stuff added later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding the finalize method\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers() ##holds all the layers that we then input using the add method\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) #adds the layer to the self.layers list\n",
    "    def set(self, *, loss, optimizer):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input() #NOT INCLUDED IN THE layer_count below\n",
    "\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        for i in range(layer_count):\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1] \n",
    "            \n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "    #ADDED this method\n",
    "    def train(self, X, y, *, epochs = 1, print_every = 1):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "\n",
    "            output = self.forward(X) # call the forward method of the model class\n",
    "\n",
    "            print(output)\n",
    "            exit()\n",
    "    #ADDED this method\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.input_layer.forward(X) #call the forward method of the actual layer class\n",
    "\n",
    "        for layer in self.layers: #for each layer in our list of layers\n",
    "            #this calls the forward function of the layer object\n",
    "            layer.forward(layer.prev.output) #why we needed the new class Layer Input. So that we could call the output attribute of that layer \n",
    "        \n",
    "        #last layer iterated in the for loop (aka the output layer)\n",
    "        return layer.output #see the layer variable in the foor loop. Our final layer in the list is assigned as layer through the for loop, so can just call it\n",
    "        #the loss is calculated in the training method above - not shown for this step currently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
