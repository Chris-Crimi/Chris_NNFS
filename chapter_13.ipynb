{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning Dataset\n",
    "- before feeding data to neural net, can perform preprocessing (needs to be done to all data, training, validation, testing (obviously))\n",
    "- NNs perform best on data that ranges from 0 to 1 or -1 to 1\n",
    "- Many very large, very small, or values of varying scale can cause the following issues\n",
    "1) if using activation functions such as tanh (values range from -1 to 1) or sigmoid (0 to 1)with very large/small values, it can cause issues with gradient calculations, because those functions are flat at their extremes, so these very high/very low values will push the gradients to extremes where these functions are flat(this is called saturation) and will make the gradients very small so there is no updates, called vanishing gradient. On the other hand, very large/small values can generally drive very large/small gradients casuing vanishing gradient or graident explosion (very large gradient making network unstable)\n",
    "2) Very large/small values can lead to overflow/underflow errors when they are constantly multiplied together throughout the network\n",
    "3) Scaled values can help speed up convergence, because network does not have to optimize vastly different values\n",
    "4) scaling values can be thought as a form of regularization by constrining extreme outliers into the -1/0/1 range, helping the network to converge faster\n",
    "- there are many ways to preprocess data - standardization, scaling, variance scaling, mean removal,non-linear transforms, scaling to outliers, etc. These are not covered in the book. The only form of preprocessing we will do in the book is dividing inputs by their maximum absolute values, like if values range from 0 to 255, then divide all by 255. We call this the scalar, referenced below\n",
    "\n",
    "Finer points\n",
    "- when dividing the training, validation, and testing datasets by the scalar, must also remember to scale the input values for when we are doing inference. So need to save whatever scaling and preprocessing we have done in addition to the model\n",
    "- if the values in prediction are slightly different but the scalar or preprocessing in general still results in values very close to the training/testing range then this is ok\n",
    "- however, if using some for of non linear scaling across training/validation/testing datasets, must be wary of information leakage. For example, if we use some values from the testing dataset to scale the training dataset, this may leak info about the test set to the training\n",
    "- data augmentation - can use this when do not have many training samples; best example of this is image processing. Can take some starting images, and then scale, rotate, etc. and add these transformations of the images to the dataset in order to create more training samples. This works best when the augmentations are similar to variations that could be seen in reality so need to consider the reality of the ground truth data once the model is used in inference relative to the augmentation.\n",
    "\n",
    "How much data do you need?\n",
    "- it depends: model size, amount of classes that will be predicted (i.e. data complexity)\n",
    "- generally, best to have a few thousand or tens of thousands of samples for each class.\n",
    "- for example, predicting two classes with a couple of input features may only requres a few hundred. Many features and few classes could require tens of thousands, and somthing like an LLM will require millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
