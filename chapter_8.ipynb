{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients, Partial Derivatives, and the Chain Rule\n",
    "-  need to calculate the derivates of the whole model including loss function with respect to each parameter (e.g., weight/bias, input)\n",
    "- need to make use of the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Partial Derivative\n",
    "- firstly, taking the partial derivative of the network function with respect to each input\n",
    "- the derivative with respect to one input is one equation\n",
    "- full function's derivatives consists of a set of equations called the gradient; it is a vector of the size of the inputs containing partial derivative solutions with respect to each of the inputs\n",
    "- finding the impact of an input on output while treating all other inputs as constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Partial Derivative of a Sum\n",
    "- exactly like normal derivative: sum of derivatives is derivative of sum. \n",
    "- treat all variables as constants except the variabile of interest (i.e., the variable which we are respecting in the derivative)\n",
    "- Example: 2x + 3y^2 => derivative with respect to x is 2 because the y term drops out since it is constant, just left taking derivative of 2x. Alternatively: Derivative with respect to y is 6y because 2x is constant and then just left taking derivative of y term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Partial Derivative of Multiplication\n",
    "- noting really new => treat the constant variables as constants/coefficients. Intuition is that we are only interested in the impact of the change in the respected variable, so if y*x, d wrt. x is y, numerically, changing x by 1 will result in change of y, so y is the slope of the partial derivative, when holding y constant\n",
    "- Example: d wrt. x of 3x^3*z - y^2 + 5z + 2yz = 9x^2*z; rest of terms drop out since they are constants. z in the x term remains because it is part of coefficient here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Partial Derivative of Max\n",
    "- only taking the derivative wrt. x \n",
    "- break the function into two pieces for this, one where x < 0 & x > 0. Where x > 0, f(x) = x, so f'(x) = 1; when x < 0, f(x) = 0 , so f'(x) = 0.\n",
    "- thus derivative of max wrt. x is 1 where (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient\n",
    "- gradient is a vector composed of all of the partial derivatives of an input function, where each partial derivatie is with respect to each input variable\n",
    "- example: f(x, y, z) = 3x^3*z - y^2 + 5z + 2yz; gradient = [dx, dy, dz] = [9x^2z, -2y + 2z, 3x^3 + 5 + 2y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chain Rule\n",
    "- use in situation where you have a nested/chain function => g(f(x)), x impacts g via f(x)\n",
    "- need this because neural network is many nested functions, where x in is the input data\n",
    "- Derivative of a chained function is the derivative of the inner function with respect to the variable times the derivative of the outer function with respect to the inner function. \n",
    "- d/dx f(g(x)) = d/dg(x) f(g(x)) * d/dx g(x) = f'(g(x)) * g'(x)\n",
    "- remember from math class: this is the u substitution, so substitute u for the inner function when taking the derivative of the outer function\n",
    "\n",
    "- please see below in the \"Why is it multiplication\" for further intutition\n",
    "\n",
    "- this applies to to multiple function layers and partial derivatives so if you have z(x) = g(h(y, f(x,z))); dz/dx = dg/dh g(h(y, f(x,z)))/h(y, f(x,z)) *   dh/df h(y, f(x,z)) / f(x,z) * df/dx f(x,z); note that did not take deriv with respect to y as it is treated as constant in this rule. We are interested in change with respect to x\n",
    "\n",
    "\n",
    "\n",
    "Why is it multiplication:\n",
    "- you have a big function that is composed of two nested medium and small functions, and you want to know the derivative of that big function with respect to the input\n",
    "- when the input goes into the small function, the small function changes the value of the input at some rate, the small rate\n",
    "- when the output of the small function goes into the medium function, the medium function changes that input at the medium rate\n",
    "- so overall, the rate that the big function changes with respect to the input is the rate that the small function changes the input times the rate that the medium function function changes the value of the small function\n",
    "- for example, if you have a function where the slope is increasing (derivative is not constant), and then you insert a function into that line that increases at a constant rate (derivative is constant), any input will move you to a steeper point on the first line than had you not had the inner function\n",
    "- more example: g = 2x h(x) = 2*g(x)^2; dh(x)/dx = 4*g(x) * 2 = 4*(2x) * 2 = 16x; x changes at a rate of 2, then whatever that value of g(x) is has an instantanuos rate of change of 4 in the larger y function. \n",
    "- another way: h(x) = g(f(x)) => g'x(fx) is only the derivative of h(x) with respect to to f(x), however if you want the h'(x) wrt to x, then need to also capture the impact that f(x) has on x. So from the example of above f(x) has rate of change of 2 on x. then g(x) has 4*g(x) rate of change on the output of f(x). So to get full rate of change on x, need to know how much x changes with f(x) and how much the outcome of that function changes with g(x). So the overall change is multiplied because the nested rate are multiplicative (e.g., doubling something and then tripling it is 6x impact)( slopes are scalars)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
