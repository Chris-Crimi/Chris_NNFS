{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 and L2 Regularization\n",
    "- the goal of regularization is to prevent weights and biases from becoming too high by penalizing large weights and biases. A very high weight indicates a neuron may be trying to memorize a feature, based on the idea that its better to have many neurons contribute to the network output than a few.\n",
    "- there are two types of regularization L1 and L2, and they follow a general form of adding a penatly gradient to the gradient of loss wrt layer's weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fowrwad pass:\n",
    "- L1 - absolute value of the weights or bias times a penalty hyperparamter lamda added to the loss function. In our implementation, we sum all the weights/bias for a layer and multiply the lambda hyper parameter to that sum. Just increases loss by the abs value of the weight/bias. When taking derivative, this just causes the derivative of loss wrt to the weights/bias for each weight/bias in that layer to increase linearlly by lambda. Backward pass discussion to follow. Formula = lambda * sum(abs(weights)); weights = weights in layer, can replace weights with bias\n",
    "- L2 - squared value of the weight/bias. This is summed up for all the weights/bias in layer. Adds penatly that scales with the weight, penalizing larger weights, and not affecting smaller weights as much. 2 x lambda x weight (or bias) impact on loss. Formula = lambda * sum(weights^2); weights = weights in layer, can replace weights with bias\n",
    "- the non-linear impact of L2 regularization impacts smaller weights less and larger weights more. the linear nature of L1 regularization impacts small weights more and can cause a model to become invariant to small values, and variant only to larger values. (i.e., because you are adding a constant value in L1, it is relatively larger to smaller weight values, having a much greater impact on pushing them to 0). As such L2 regularization is used more often than L1, and L1 is rarely used on its own.\n",
    "- lambda is the penalty scalar that dicates how strong the penalty is\n",
    "- in our implementation we set lambda independently for each layer. Then we add each penalty to the loss value. This is a more efficient solution, per chat GPT, as it would be a lot to set individual lambdas. It is simpler and more interpretable.\n",
    "- regularization drives model parameters closer to 0, thus accomplishing the goal of forcing network to have weight/bias parameters too high. NEED MORE EXPLAIN AROUND THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization losses for both L1 and L2 are accumulated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
